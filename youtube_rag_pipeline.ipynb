{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6c0886a0",
      "metadata": {
        "id": "6c0886a0"
      },
      "source": [
        "# YouTube RAG Pipeline\n",
        "\n",
        "Pipeline\n",
        "1. **Loading Data**\n",
        "2. **Chunking**\n",
        "3. **Embeddings** (OpenAI-Embeddings)\n",
        "4. **VectorDB** (Chroma)\n",
        "5. **Retriever**\n",
        "6. **LLM** (OpenAI Chat-Model)\n",
        "7. **Chain** (Conversational Retrieval)\n",
        "8. **Memory**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EAj4VFl7sBRz"
      },
      "id": "EAj4VFl7sBRz"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langchain langchain-openai langchain-community langchain-text-splitters chromadb tiktoken python-dotenv"
      ],
      "metadata": {
        "id": "bqR6EgEZuYkq"
      },
      "id": "bqR6EgEZuYkq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n"
      ],
      "metadata": {
        "id": "Nw2ZkllvueXp"
      },
      "id": "Nw2ZkllvueXp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "    print(\"✅ API Key loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ6_OV_Lsvp7",
        "outputId": "be869e42-2831-4b7a-d187-071fcf2468b8"
      },
      "id": "VZ6_OV_Lsvp7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API Key loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64b888d7",
      "metadata": {
        "id": "64b888d7"
      },
      "source": [
        "## 1. Loading Data\n",
        "\n",
        "Lege deine Transkript-Dateien als **`.txt`** in einen Ordner, z.B. `data/`.\n",
        "Jede Datei kann z.B. der Transkripttext eines YouTube-Videos sein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60cdbc46",
      "metadata": {
        "id": "60cdbc46"
      },
      "outputs": [],
      "source": [
        "# Ordner für Transkripte\n",
        "DATA_DIR = Path(\"data\")  # passe den Pfad an, falls nötig\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def load_transcripts(data_dir: Path):\n",
        "    docs = []\n",
        "    for path in data_dir.glob('*.txt'):\n",
        "        with path.open('r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        if not text.strip():\n",
        "            continue\n",
        "        docs.append(\n",
        "            Document(\n",
        "                page_content=text,\n",
        "                metadata={\"source\": path.name}\n",
        "            )\n",
        "        )\n",
        "    return docs\n",
        "\n",
        "documents = load_transcripts(DATA_DIR)\n",
        "print(f\"Geladene Dokumente: {len(documents)}\")\n",
        "if documents:\n",
        "    print(\"Beispiel-Dokument:\")\n",
        "    print(\"Quelle:\", documents[0].metadata)\n",
        "    print(textwrap.shorten(documents[0].page_content, width=400, placeholder=\" ...\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a79ca3",
      "metadata": {
        "id": "91a79ca3"
      },
      "source": [
        "## 2. Chunking\n",
        "\n",
        "Wir zerlegen die langen Transkripte in kleinere Textstücke (Chunks), damit die Vektor-Suche besser funktioniert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddca5111",
      "metadata": {
        "id": "ddca5111"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Größe eines Chunks (Tokens/Zeichen-Nähe)\n",
        "    chunk_overlap=200,    # Überlappung, damit der Kontext nicht abreißt\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Anzahl Chunks: {len(splits)}\")\n",
        "if splits:\n",
        "    print(\"Beispiel-Chunk:\")\n",
        "    print(\"Quelle:\", splits[0].metadata)\n",
        "    print(textwrap.shorten(splits[0].page_content, width=300, placeholder=\" ...\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4f8b76",
      "metadata": {
        "id": "cb4f8b76"
      },
      "source": [
        "## 3. Embeddings & 4. VectorDB (Chroma)\n",
        "\n",
        "Wir erzeugen Embeddings mit einem OpenAI-Embedding-Modell und speichern diese in einer Chroma-Datenbank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79095ff8",
      "metadata": {
        "id": "79095ff8"
      },
      "outputs": [],
      "source": [
        "# Embedding-Modell\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# Chroma-DB erstellen\n",
        "PERSIST_DIR = \"chroma_db\"\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=PERSIST_DIR\n",
        ")\n",
        "\n",
        "vectordb.persist()\n",
        "print(\"✅ Vektor-Datenbank erstellt und gespeichert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a67cdc9e",
      "metadata": {
        "id": "a67cdc9e"
      },
      "source": [
        "## 5. Retriever\n",
        "\n",
        "Der Retriever holt zu einer Frage die **relevantesten Chunks** aus der VectorDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e977cb00",
      "metadata": {
        "id": "e977cb00"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "print(\"✅ Retriever bereit.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7de9142",
      "metadata": {
        "id": "b7de9142"
      },
      "source": [
        "## 6. LLM, 7. Chain & 8. Memory\n",
        "\n",
        "Wir nutzen ein schnelles OpenAI-Chat-Modell (`gpt-4.1-mini`) und bauen eine\n",
        "**ConversationalRetrievalChain**, die auch einen Chat-Verlauf (Memory) verwendet.\n",
        "\n",
        "Der Chat-Verlauf wird in einer Python-Liste `chat_history` gespeichert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2dab9e",
      "metadata": {
        "id": "3c2dab9e"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",  # gutes Preis/Leistungs-Verhältnis\n",
        "    temperature=0.2,        # eher sachlich\n",
        ")\n",
        "\n",
        "# Conversational Retrieval Chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")\n",
        "\n",
        "# Einfache Memory-Struktur: Liste aus (user, assistant)-Nachrichten\n",
        "chat_history = []\n",
        "\n",
        "def ask(question: str):\n",
        "    \"\"\"Stellt eine Frage an die RAG-Chain und aktualisiert den Chat-Verlauf.\"\"\"\n",
        "    global chat_history\n",
        "    result = qa_chain({\n",
        "        \"question\": question,\n",
        "        \"chat_history\": chat_history,\n",
        "    })\n",
        "\n",
        "    answer = result[\"answer\"]\n",
        "    source_docs = result.get(\"source_documents\", [])\n",
        "\n",
        "    # Chat-Verlauf updaten\n",
        "    chat_history.append((question, answer))\n",
        "\n",
        "    # Antwort + Quellen hübsch ausgeben\n",
        "    print(\"Frage:\\n\", question)\n",
        "    print(\"\\nAntwort:\\n\", textwrap.fill(answer, width=100))\n",
        "    if source_docs:\n",
        "        print(\"\\nVerwendete Quellen:\")\n",
        "        for i, d in enumerate(source_docs, 1):\n",
        "            short = textwrap.shorten(d.page_content, width=120, placeholder=\" ...\")\n",
        "            print(f\"[{i}] {d.metadata.get('source')} :: {short}\")\n",
        "\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2cb06bc",
      "metadata": {
        "id": "a2cb06bc"
      },
      "source": [
        "## 9. Test: Frage an dein RAG-System stellen\n",
        "\n",
        "Jetzt kannst du dein System testen. Formuliere eine Frage, die im Inhalt deiner Transkripte beantwortet werden kann."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9683395",
      "metadata": {
        "id": "c9683395"
      },
      "outputs": [],
      "source": [
        "# Beispiel-Frage (anpassen!)\n",
        "example_question = \"Worum geht es in dem ersten Video grob?\"\n",
        "ask(example_question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}