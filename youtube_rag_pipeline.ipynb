{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6c0886a0",
      "metadata": {
        "id": "6c0886a0"
      },
      "source": [
        "# YouTube RAG Pipeline\n",
        "\n",
        "Pipeline\n",
        "1. **Loading Data**\n",
        "2. **Chunking**\n",
        "3. **Embeddings** (OpenAI-Embeddings)\n",
        "4. **VectorDB** (Chroma)\n",
        "5. **Retriever**\n",
        "6. **LLM** (OpenAI Chat-Model)\n",
        "7. **Chain** (Conversational Retrieval)\n",
        "8. **Memory**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. YouTube videos / transcripts"
      ],
      "metadata": {
        "id": "l2dqM0Be1PI-"
      },
      "id": "l2dqM0Be1PI-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "fHjZjvQ21l3C"
      },
      "id": "fHjZjvQ21l3C"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y youtube-transcript-api\n",
        "!pip install youtube-transcript-api==0.6.2"
      ],
      "metadata": {
        "id": "dH-cUoE1unK7"
      },
      "id": "dH-cUoE1unK7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata  # this is what reads Colab secrets\n",
        "\n",
        "# Get the key from Colab Secrets\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if openai_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in Colab secrets. Check the name.\")\n",
        "\n",
        "# Option A: set as environment variable so LangChain & others can use it\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "\n",
        "print(\"Loaded OPENAI_API_KEY from Colab secrets:\", os.environ[\"OPENAI_API_KEY\"] is not None)\n"
      ],
      "metadata": {
        "id": "ZI3mea0jpm94"
      },
      "id": "ZI3mea0jpm94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api chromadb sentence-transformers transformers accelerate torch\"\"\""
      ],
      "metadata": {
        "id": "rR0yL9Me1vP-"
      },
      "id": "rR0yL9Me1vP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "from youtube_transcript_api import (\n",
        "    YouTubeTranscriptApi,\n",
        "    TranscriptsDisabled,\n",
        "    NoTranscriptFound,\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions"
      ],
      "metadata": {
        "id": "eXO5JhUm1egX"
      },
      "id": "eXO5JhUm1egX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YouTube ingestion\n",
        "- URL -> video_id\n",
        "- video_id -> transcript (list)\n",
        "- transcript -> plain text"
      ],
      "metadata": {
        "id": "qFHwe7lD2TjT"
      },
      "id": "qFHwe7lD2TjT"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_video_id(url: str) -> str:\n",
        "    \"\"\"Extract YouTube video ID from normal or short URLs.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "\n",
        "    # Short youtu.be links\n",
        "    if parsed.netloc in (\"youtu.be\", \"www.youtu.be\"):\n",
        "        return parsed.path.lstrip(\"/\")\n",
        "\n",
        "    # Regular youtube.com links\n",
        "    if parsed.netloc in (\"www.youtube.com\", \"youtube.com\", \"m.youtube.com\"):\n",
        "        qs = parse_qs(parsed.query)\n",
        "        vid = qs.get(\"v\", [None])[0]\n",
        "        if vid:\n",
        "            return vid\n",
        "\n",
        "    raise ValueError(f\"Could not extract video_id from URL: {url}\")\n",
        "\n",
        "\n",
        "def transcript_to_text(transcript, include_timestamps: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Convert a list of segments from youtube_transcript_api into plain text.\n",
        "    transcript: [{'text': '...', 'start': 0.0, 'duration': 3.2}, ...]\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for seg in transcript:\n",
        "        text = seg.get(\"text\", \"\")\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        if include_timestamps:\n",
        "            start = seg.get(\"start\", 0)\n",
        "            minutes = int(start // 60)\n",
        "            seconds = int(start % 60)\n",
        "            timestamp = f\"[{minutes:02d}:{seconds:02d}] \"\n",
        "            lines.append(timestamp + text)\n",
        "        else:\n",
        "            lines.append(text)\n",
        "\n",
        "    return \" \".join(lines)\n",
        "\n",
        "\n",
        "def fetch_transcript_text(\n",
        "    video_id: str,\n",
        "    preferred_languages=None,\n",
        "    include_timestamps: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Try to fetch a transcript:\n",
        "    1) in preferred_languages (if provided)\n",
        "    2) otherwise in English-ish defaults\n",
        "    3) if that fails, fall back to ANY available transcript\n",
        "    \"\"\"\n",
        "    if preferred_languages is None:\n",
        "        preferred_languages = [\"en\", \"en-US\", \"en-GB\"]\n",
        "\n",
        "    # Normalize to list of str\n",
        "    if isinstance(preferred_languages, str):\n",
        "        preferred_languages = [\n",
        "            lang.strip() for lang in preferred_languages.split(\",\") if lang.strip()\n",
        "        ]\n",
        "\n",
        "    try:\n",
        "        # First: try preferred languages\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(\n",
        "                video_id,\n",
        "                languages=preferred_languages,\n",
        "            )\n",
        "            return transcript_to_text(transcript, include_timestamps=include_timestamps)\n",
        "\n",
        "        except NoTranscriptFound:\n",
        "            print(\n",
        "                f\"  No transcript in preferred languages {preferred_languages} \"\n",
        "                f\"for {video_id}. Trying any available transcript...\"\n",
        "            )\n",
        "\n",
        "            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "            # just pick the first available transcript\n",
        "            t = next(iter(transcript_list))\n",
        "            transcript = t.fetch()\n",
        "            return transcript_to_text(transcript, include_timestamps=include_timestamps)\n",
        "\n",
        "    except TranscriptsDisabled:\n",
        "        raise RuntimeError(f\"Transcripts are DISABLED for video_id={video_id}\")\n",
        "    except NoTranscriptFound:\n",
        "        raise RuntimeError(\n",
        "            f\"No transcript found at all for video_id={video_id} \"\n",
        "            f\"(even after fallback).\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error fetching transcript for {video_id}: {e}\")\n",
        "\n",
        "\n",
        "def ingest_youtube_videos(\n",
        "    urls,\n",
        "    languages=None,  # passed to preferred_languages\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For each URL:\n",
        "    - extract video_id\n",
        "    - fetch transcript\n",
        "    - store in DataFrame\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for url in urls:\n",
        "        print(f\"\\n=== Processing URL: {url} ===\")\n",
        "        try:\n",
        "            video_id = extract_video_id(url)\n",
        "            print(f\"  video_id: {video_id}\")\n",
        "\n",
        "            transcript = fetch_transcript_text(\n",
        "                video_id,\n",
        "                preferred_languages=languages,\n",
        "            )\n",
        "            print(f\"  ✅ Transcript length: {len(transcript)} characters\")\n",
        "\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"video_id\": video_id,\n",
        "                    \"url\": url,\n",
        "                    \"transcript\": transcript,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ Skipping {url}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    print(f\"\\nIngested {len(df)} videos out of {len(urls)} URLs.\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "Bs8Ik8rc2fwc"
      },
      "id": "Bs8Ik8rc2fwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store transcripts in Chroma"
      ],
      "metadata": {
        "id": "c_LlEnne4kxE"
      },
      "id": "c_LlEnne4kxE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Chroma collection from a DataFrame with columns 'video_id', 'url', 'transcript'\n",
        "def build_chroma_collection_from_df(\n",
        "    df: pd.DataFrame,\n",
        "    collection_name: str = \"youtube_videos\",\n",
        "):\n",
        "    # 2.1 Set up Chroma client (in-memory for now; for persistence use PersistentClient)\n",
        "    client = chromadb.Client()\n",
        "\n",
        "    # 2.2 Define an embedding function (SentenceTransformer)\n",
        "    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "        model_name=\"all-MiniLM-L6-v2\"\n",
        "    )\n",
        "\n",
        "    # 2.3 Create (or recreate) the collection\n",
        "    existing = [c.name for c in client.list_collections()]\n",
        "    if collection_name in existing:\n",
        "        client.delete_collection(collection_name)\n",
        "\n",
        "    collection = client.create_collection(\n",
        "        name=collection_name,\n",
        "        embedding_function=embedding_func,\n",
        "    )\n",
        "\n",
        "    # 2.4 Add documents to collection\n",
        "    documents = df[\"transcript\"].tolist()\n",
        "    ids = df[\"video_id\"].tolist()\n",
        "    metadatas = df[[\"video_id\", \"url\"]].to_dict(orient=\"records\")\n",
        "\n",
        "    collection.add(\n",
        "        documents=documents,\n",
        "        ids=ids,\n",
        "        metadatas=metadatas,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Added {len(documents)} transcripts to Chroma collection \"\n",
        "        f\"'{collection_name}'.\"\n",
        "    )\n",
        "    return collection"
      ],
      "metadata": {
        "id": "b8ST4sVL4opB"
      },
      "id": "b8ST4sVL4opB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcribe"
      ],
      "metadata": {
        "id": "s6UWolFA6cv3"
      },
      "id": "s6UWolFA6cv3"
    },
    {
      "cell_type": "code",
      "source": [
        "video_urls = [\n",
        "        \"https://www.youtube.com/watch?v=enD8mK9Zvwo\",\n",
        "        \"https://www.youtube.com/watch?v=ZdjJdoEwCY4\",\n",
        "    ]\n",
        "\n",
        "    df_videos = ingest_youtube_videos(video_urls, languages=[\"en\"])\n",
        "\n",
        "    print(\"\\nIngested videos DataFrame:\")\n",
        "    print(df_videos.head())\n",
        "\n",
        "    if df_videos.empty:\n",
        "        print(\"❌ No videos ingested successfully – check URLs or transcripts settings.\")\n",
        "        # raise SystemExit(\"No videos ingested successfully – check URLs or transcripts settings.\")\n",
        "    else:\n",
        "        print(\"✅ At least one transcript ingested.\")"
      ],
      "metadata": {
        "id": "TB4vj0G-6cAW"
      },
      "id": "TB4vj0G-6cAW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_videos.head()"
      ],
      "metadata": {
        "id": "FV1jSMsL-bgG"
      },
      "id": "FV1jSMsL-bgG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "URL ingestion for youtube links"
      ],
      "metadata": {
        "id": "Hy0oSolze_VB"
      },
      "id": "Hy0oSolze_VB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Chunking + LangChain Documents on Top the Your DataFrame"
      ],
      "metadata": {
        "id": "6Cc_3H_tg-PY"
      },
      "id": "6Cc_3H_tg-PY"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai langchain-community langsmith"
      ],
      "metadata": {
        "id": "79oO2obyfFCZ"
      },
      "id": "79oO2obyfFCZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain langchain-openai langchain-core langchain-community\n",
        "!pip install pytube\n"
      ],
      "metadata": {
        "id": "ypllD4YghaWk"
      },
      "id": "ypllD4YghaWk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from pytube import YouTube  # to enrich metadata from YouTube\n",
        "\n",
        "\n",
        "# Convert each row in df_videos (video_id, url, transcript) into multiple LangChain Documents with metadata.\n",
        "def df_to_documents(\n",
        "    df: pd.DataFrame,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 150,\n",
        ") -> list[Document]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "\n",
        "    docs: list[Document] = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        video_id = row[\"video_id\"]\n",
        "        url = row[\"url\"]\n",
        "        transcript = row[\"transcript\"]\n",
        "\n",
        "        # Split transcript into chunks\n",
        "        chunks = splitter.split_text(transcript)\n",
        "\n",
        "        # Try to get richer metadata from YouTube, but fail gracefully\n",
        "        title = \"\"\n",
        "        author = \"\"\n",
        "        description = \"\"\n",
        "        try:\n",
        "            yt = YouTube(url)\n",
        "            title = yt.title or \"\"\n",
        "            author = yt.author or \"\"\n",
        "            description = yt.description or \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"Could not fetch metadata for {url}: {e}\")\n",
        "\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            doc = Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"video_id\": video_id,\n",
        "                    \"url\": url,\n",
        "                    \"title\": title,\n",
        "                    \"author\": author,\n",
        "                    \"description\": description,\n",
        "                    \"chunk_index\": idx,\n",
        "                },\n",
        "            )\n",
        "            docs.append(doc)\n",
        "\n",
        "    return docs"
      ],
      "metadata": {
        "id": "DQAyQkdpg9LU"
      },
      "id": "DQAyQkdpg9LU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df_to_documents(df_videos)\n",
        "print(f\"Created {len(documents)} chunks from {len(df_videos)} videos.\")"
      ],
      "metadata": {
        "id": "SRPjokEDhUVi"
      },
      "id": "SRPjokEDhUVi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a LangChain VectorStore (Chroma) from Documents"
      ],
      "metadata": {
        "id": "ZAM2QMuAmBjJ"
      },
      "id": "ZAM2QMuAmBjJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings  # requires OPENAI_API_KEY\n",
        "\n",
        "def build_vectorstore_from_documents(\n",
        "    docs: list[Document],\n",
        "    collection_name: str = \"youtube_rag\",\n",
        "    persist_directory: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a Chroma vector store from LangChain Documents.\n",
        "    Uses OpenAI embeddings by default.\n",
        "    \"\"\"\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "        collection_name=collection_name,\n",
        "        persist_directory=persist_directory,\n",
        "    )\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "23CYzLAnl0ly"
      },
      "id": "23CYzLAnl0ly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df_to_documents(df_videos)\n",
        "vectorstore = build_vectorstore_from_documents(\n",
        "    documents,\n",
        "    collection_name=\"youtube_rag\",\n",
        "    persist_directory=\"./chroma_youtube_rag\",\n",
        ")"
      ],
      "metadata": {
        "id": "TOeHGCPpmCkR"
      },
      "id": "TOeHGCPpmCkR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fC1ZQLxdmQxQ"
      },
      "id": "fC1ZQLxdmQxQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}