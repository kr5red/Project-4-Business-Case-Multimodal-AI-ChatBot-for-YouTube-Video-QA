{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kr5red/Project-4-Business-Case-Multimodal-AI-ChatBot-for-YouTube-Video-QA/blob/main/main_version5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0886a0",
      "metadata": {
        "id": "6c0886a0"
      },
      "source": [
        "# YouTube RAG Pipeline\n",
        "\n",
        "0. Installments & Imports\n",
        "1. Ingest YouTube Videos → DataFrame\n",
        "2. Convert Transcripts → LangChain Documents\n",
        "3. Build Vector Store (Chroma + OpenAI Embeddings)\n",
        "4. (Next Steps – Implemented in Later Cells)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Installments & Imports"
      ],
      "metadata": {
        "id": "l2dqM0Be1PI-"
      },
      "id": "l2dqM0Be1PI-"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api chromadb pytube\n",
        "!pip install -q -U langchain langchain-openai langchain-core langchain-community langsmith\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLyu0wulxdWa",
        "outputId": "00461dc3-a3bf-43a5-d3cc-8366b1dee6bb"
      },
      "id": "DLyu0wulxdWa",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.12/dist-packages (15.0.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "# Colab secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain imports (modern API)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import Tool, tool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "\n",
        "# Text splitting + Documents\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# VectorDB\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# YouTube transcript\n",
        "from youtube_transcript_api import (\n",
        "    YouTubeTranscriptApi,\n",
        "    TranscriptsDisabled,\n",
        "    NoTranscriptFound,\n",
        ")\n",
        "\n",
        "# Metadata enrichment\n",
        "from pytube import YouTube\n",
        "\n",
        "#Speech recognition\n",
        "from google.colab import files\n",
        "from openai import OpenAI\n",
        "import uuid\n",
        "\n",
        "#Tools\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "#deployment\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "eXO5JhUm1egX"
      },
      "id": "eXO5JhUm1egX",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "langchain_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "if openai_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in Colab secrets.\")\n",
        "if langchain_key is None:\n",
        "    raise ValueError(\"LANGCHAIN_API_KEY not found in Colab secrets.\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"    # required for LangSmith\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"youtube-qa-bot\"\n",
        "\n",
        "# Reusable OpenAI client for audio transcription etc.\n",
        "openai_client = OpenAI()\n",
        "\n",
        "print(\"Keys loaded\")\n",
        "print(\"LangSmith enabled — project:\", os.environ[\"LANGCHAIN_PROJECT\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL_s4k7aZnDL",
        "outputId": "46382972-d5d5-46f2-d253-e22a4a6a6bcf"
      },
      "id": "HL_s4k7aZnDL",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys loaded\n",
            "LangSmith enabled — project: youtube-qa-bot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YouTube ingestion\n",
        "- URL -> video_id\n",
        "- video_id -> transcript (list)\n",
        "- transcript -> plain text"
      ],
      "metadata": {
        "id": "qFHwe7lD2TjT"
      },
      "id": "qFHwe7lD2TjT"
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the YouTube video ID from URL formats\n",
        "def extract_video_id(url: str) -> str:\n",
        "    parsed = urlparse(url)\n",
        "\n",
        "    # Short youtu.be links\n",
        "    if parsed.netloc in (\"youtu.be\", \"www.youtu.be\"):\n",
        "        return parsed.path.lstrip(\"/\")\n",
        "\n",
        "    # Regular youtube.com links\n",
        "    if parsed.netloc in (\"www.youtube.com\", \"youtube.com\", \"m.youtube.com\"):\n",
        "        qs = parse_qs(parsed.query)\n",
        "        vid = qs.get(\"v\", [None])[0]\n",
        "        if vid:\n",
        "            return vid\n",
        "\n",
        "    raise ValueError(f\"Could not extract video_id from URL: {url}\")\n",
        "\n",
        "#Convert a transcript (list of {text, start, duration}) to a single text string\n",
        "def transcript_to_text(transcript, include_timestamps: bool = False) -> str:\n",
        "    lines = []\n",
        "    for entry in transcript:\n",
        "        if include_timestamps:\n",
        "            start = entry[\"start\"]\n",
        "            lines.append(f\"[{start:.1f}s] {entry['text']}\")\n",
        "        else:\n",
        "            lines.append(entry[\"text\"])\n",
        "    return \" \".join(lines)\n",
        "\n",
        "\n",
        "#Fetch transcript for a single video_id and turn it into plain text.\n",
        "def fetch_transcript_text(video_id: str, languages=None) -> str:\n",
        "    try:\n",
        "        ytt_api = YouTubeTranscriptApi()\n",
        "\n",
        "        # If you don't care about language, you can call ytt_api.fetch(video_id) without languages\n",
        "        if languages is None:\n",
        "            fetched = ytt_api.fetch(video_id)\n",
        "        else:\n",
        "            fetched = ytt_api.fetch(video_id, languages=languages)\n",
        "\n",
        "        # `fetched` is a FetchedTranscript object with `.snippets`\n",
        "        # Convert to the same structure transcript_to_text() expects\n",
        "        transcript = [\n",
        "            {\"text\": s.text, \"start\": s.start, \"duration\": s.duration}\n",
        "            for s in fetched.snippets\n",
        "        ]\n",
        "\n",
        "        return transcript_to_text(transcript, include_timestamps=False)\n",
        "\n",
        "    except TranscriptsDisabled:\n",
        "        raise RuntimeError(f\"Transcripts are disabled for video_id={video_id}\")\n",
        "    except NoTranscriptFound:\n",
        "        raise RuntimeError(f\"No transcript found for video_id={video_id} in languages={languages}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error fetching transcript for {video_id}: {e}\")\n"
      ],
      "metadata": {
        "id": "Bs8Ik8rc2fwc"
      },
      "id": "Bs8Ik8rc2fwc",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ingest YouTube videos into a DataFrame"
      ],
      "metadata": {
        "id": "c_LlEnne4kxE"
      },
      "id": "c_LlEnne4kxE"
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_youtube_videos(urls, languages=\"en, de\") -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            video_id = extract_video_id(url)\n",
        "            transcript = fetch_transcript_text(video_id, languages=languages)\n",
        "            rows.append({\n",
        "                \"video_id\": video_id,\n",
        "                \"url\": url,\n",
        "                \"transcript\": transcript,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {url}: {e}\")\n",
        "    return pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "id": "5jefkcqOe_z4"
      },
      "id": "5jefkcqOe_z4",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ingest multiple videos ----\n",
        "video_urls = [\n",
        "    \"https://www.youtube.com/watch?v=HG68Ymazo18\",\n",
        "]\n",
        "\n",
        "df_videos = ingest_youtube_videos(video_urls, languages=[\"en\"])\n",
        "\n",
        "if df_videos.empty:\n",
        "    print(\"No videos ingested ...\")\n",
        "else:\n",
        "    print(df_videos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB4vj0G-6cAW",
        "outputId": "9b113f93-e238-4d0b-9c75-4da64a8be35d"
      },
      "id": "TB4vj0G-6cAW",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping https://www.youtube.com/watch?v=HG68Ymazo18: Error fetching transcript for HG68Ymazo18: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=HG68Ymazo18! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "No videos ingested ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_videos.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "FV1jSMsL-bgG",
        "outputId": "050511c5-16d3-4aa8-932f-89920810524b"
      },
      "id": "FV1jSMsL-bgG",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-395955bb-b7d4-4275-a553-dc4ad84f2960\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-395955bb-b7d4-4275-a553-dc4ad84f2960')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-395955bb-b7d4-4275-a553-dc4ad84f2960 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-395955bb-b7d4-4275-a553-dc4ad84f2960');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_videos",
              "summary": "{\n  \"name\": \"df_videos\",\n  \"rows\": 0,\n  \"fields\": []\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Chunking + LangChain Documents on Top the Your DataFrame"
      ],
      "metadata": {
        "id": "6Cc_3H_tg-PY"
      },
      "id": "6Cc_3H_tg-PY"
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert each row in df_videos (video_id, url, transcript) into multiple LangChain Documents with metadata.\n",
        "def df_to_documents(\n",
        "    df: pd.DataFrame,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 150,\n",
        ") -> list[Document]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "\n",
        "    docs: list[Document] = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        video_id = row[\"video_id\"]\n",
        "        url = row[\"url\"]\n",
        "        transcript = row[\"transcript\"]\n",
        "\n",
        "        # Try to fetch some metadata from YouTube\n",
        "        title = author = description = None\n",
        "        try:\n",
        "            yt = YouTube(url)\n",
        "            title = yt.title\n",
        "            author = yt.author\n",
        "            description = yt.description\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Split transcript into chunks\n",
        "        chunks = splitter.split_text(transcript)\n",
        "\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            doc = Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"video_id\": video_id,\n",
        "                    \"url\": url,\n",
        "                    \"title\": title,\n",
        "                    \"author\": author,\n",
        "                    \"description\": description,\n",
        "                    \"chunk_index\": idx,\n",
        "                },\n",
        "            )\n",
        "            docs.append(doc)\n",
        "\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "DQAyQkdpg9LU"
      },
      "id": "DQAyQkdpg9LU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df_to_documents(df_videos)\n",
        "print(f\"Created {len(documents)} chunks from {len(df_videos)} videos.\")"
      ],
      "metadata": {
        "id": "SRPjokEDhUVi",
        "outputId": "6e0e46ad-3e70-4957-b6aa-d9728790564b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SRPjokEDhUVi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 6 chunks from 1 videos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a LangChain VectorStore (Chroma) from Documents"
      ],
      "metadata": {
        "id": "ZAM2QMuAmBjJ"
      },
      "id": "ZAM2QMuAmBjJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def build_vectorstore_from_documents(\n",
        "    docs: list[Document],\n",
        "    collection_name: str = \"youtube_rag\",\n",
        "    persist_directory: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a Chroma vector store from LangChain Documents.\n",
        "    Uses OpenAI embeddings by default.\n",
        "    \"\"\"\n",
        "    # OpenAI embedding model\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "        collection_name=collection_name,\n",
        "        persist_directory=persist_directory,  # can be None for in-memory\n",
        "    )\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "23CYzLAnl0ly"
      },
      "id": "23CYzLAnl0ly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = build_vectorstore_from_documents(\n",
        "    documents,\n",
        "    collection_name=\"youtube_rag\",\n",
        "    persist_directory=\"./chroma_youtube_rag\",\n",
        ")"
      ],
      "metadata": {
        "id": "TOeHGCPpmCkR"
      },
      "id": "TOeHGCPpmCkR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM + Retriever + Memory"
      ],
      "metadata": {
        "id": "Od7TS69bS2ud"
      },
      "id": "Od7TS69bS2ud"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant answering questions about the content \"\n",
        "            \"of YouTube videos indexed in a vector database. \"\n",
        "            \"Use the retrieved context to answer accurately.\"\n",
        "        ),\n",
        "        (\"human\", \"Context from videos:\\n{context}\\n\\nQuestion: {question}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Lx-_wSeDQaZG"
      },
      "id": "Lx-_wSeDQaZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the RAG pipeline manually"
      ],
      "metadata": {
        "id": "wXCjLu7RS9gP"
      },
      "id": "wXCjLu7RS9gP"
    },
    {
      "cell_type": "code",
      "source": [
        "def youtube_rag_query(question: str):\n",
        "    \"\"\"\n",
        "    Full RAG pipeline:\n",
        "    1. Retrieve relevant video chunks\n",
        "    2. Add them into the prompt\n",
        "    3. Call the LLM\n",
        "    4. Store chat history (memory)\n",
        "    \"\"\"\n",
        "    # ---- Retrieval ----\n",
        "    docs = retriever.invoke(question)\n",
        "\n",
        "    if not docs:\n",
        "        context = \"No relevant content found.\"\n",
        "    else:\n",
        "        context_parts = []\n",
        "        for i, d in enumerate(docs):\n",
        "            meta = d.metadata or {}\n",
        "            context_parts.append(\n",
        "                f\"[{i+1}] Title: {meta.get('title', 'Unknown')}\\n\"\n",
        "                f\"Channel: {meta.get('author', 'Unknown')}\\n\"\n",
        "                f\"Description: {meta.get('description', 'No description')}\\n\"\n",
        "                f\"Chunk {meta.get('chunk_index', '?')}:\\n{d.page_content}\"\n",
        "            )\n",
        "        context = \"\\n\\n---\\n\".join(context_parts)\n",
        "\n",
        "    # ---- Build prompt ----\n",
        "    prompt_msg = rag_prompt.format_messages(\n",
        "        context=context,\n",
        "        question=question,\n",
        "    )\n",
        "\n",
        "    # ---- LLM call ----\n",
        "    response = llm.invoke(prompt_msg)\n",
        "\n",
        "    # ---- Memory update ----\n",
        "    memory.add_message(HumanMessage(content=question))\n",
        "    memory.add_message(response)\n",
        "\n",
        "    return response.content, context\n"
      ],
      "metadata": {
        "id": "EB5XMUeFS5r9"
      },
      "id": "EB5XMUeFS5r9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tools\n",
        "\n",
        "@tool\n",
        "def youtube_rag_qa(question: str) -> str:\n",
        "    \"\"\"Answer questions about the YouTube videos that have been ingested into the vector store.\"\"\"\n",
        "    answer, _ctx = youtube_rag_query(question)\n",
        "    return answer\n",
        "\n",
        "@tool\n",
        "def transcribe_audio_file(audio_path: str) -> str:\n",
        "    \"\"\"Transcribe an audio file at a local path to text.\"\"\"\n",
        "    return transcribe_audio_to_text(audio_path)\n",
        "\n",
        "tools = [youtube_rag_qa, transcribe_audio_file]\n",
        "tool_map = {t.name: t for t in tools}\n",
        "\n",
        "# LLM that can call tools\n",
        "tool_llm = llm.bind_tools(tools)\n",
        "\n",
        "# ---- Conversation memory (per session) ----\n",
        "\n",
        "session_history: Dict[str, List[BaseMessage]] = defaultdict(list)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an assistant that answers questions about a set of YouTube videos \"\n",
        "    \"that have already been ingested into a vector database.\\n\"\n",
        "    \"If the user asks about the content of the videos, you should use the \"\n",
        "    \"`youtube_rag_qa` tool.\\n\"\n",
        "    \"If the user gives or refers to an audio file path, you can use the \"\n",
        "    \"`transcribe_audio_file` tool to turn it into text and then use \"\n",
        "    \"`youtube_rag_qa` with the transcribed question.\\n\"\n",
        "    \"Always give concise, helpful answers.\"\n",
        ")\n",
        "\n",
        "def agent_chat(user_input: str, session_id: str = \"default\") -> str:\n",
        "    \"\"\"\n",
        "    Simple agent:\n",
        "    - Uses OpenAI tool-calling to decide whether to call youtube_rag_qa / transcribe_audio_file\n",
        "    - Keeps per-session memory of previous turns\n",
        "    \"\"\"\n",
        "    history = session_history[session_id]\n",
        "\n",
        "    # 1) Build the message list: system + history + new user message\n",
        "    messages: List[BaseMessage] = [\n",
        "        SystemMessage(content=SYSTEM_PROMPT),\n",
        "        *history,\n",
        "        HumanMessage(content=user_input),\n",
        "    ]\n",
        "\n",
        "    # 2) First model call: the model may decide to call tools\n",
        "    ai_msg = tool_llm.invoke(messages)\n",
        "    messages.append(ai_msg)\n",
        "\n",
        "    # If the model already answered without tools, just return that\n",
        "    if not getattr(ai_msg, \"tool_calls\", None):\n",
        "        history.extend([HumanMessage(content=user_input), ai_msg])\n",
        "        return ai_msg.content\n",
        "\n",
        "    # 3) If there are tool calls, execute them\n",
        "    tool_messages: List[ToolMessage] = []\n",
        "    for call in ai_msg.tool_calls:\n",
        "        tool_name = call[\"name\"]\n",
        "        tool_args = call[\"args\"]\n",
        "        tool_id = call[\"id\"]\n",
        "\n",
        "        tool = tool_map.get(tool_name)\n",
        "        if tool is None:\n",
        "            tool_result = f\"Error: tool '{tool_name}' not found.\"\n",
        "        else:\n",
        "            # invoke() expects a dict of arguments for the tool\n",
        "            tool_result = tool.invoke(tool_args)\n",
        "\n",
        "        tool_messages.append(\n",
        "            ToolMessage(\n",
        "                content=str(tool_result),\n",
        "                tool_call_id=tool_id,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # 4) Second model call: model sees tool results and produces final answer\n",
        "    messages.extend(tool_messages)\n",
        "    final_ai = llm.invoke(messages)\n",
        "\n",
        "    # 5) Update history with just the human input and final answer\n",
        "    history.extend([\n",
        "        HumanMessage(content=user_input),\n",
        "        final_ai,\n",
        "    ])\n",
        "\n",
        "    return final_ai.content\n"
      ],
      "metadata": {
        "id": "9QFEEFeZa4Ix"
      },
      "id": "9QFEEFeZa4Ix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the RAG pipeline"
      ],
      "metadata": {
        "id": "yu83nRnXuQbd"
      },
      "id": "yu83nRnXuQbd"
    },
    {
      "cell_type": "code",
      "source": [
        "answer, used_context = youtube_rag_query(\"Give me an overview of the videos you indexed.\")\n",
        "\n",
        "print(\"=== ANSWER ===\\n\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\n=== CONTEXT USED ===\\n\")\n",
        "print(used_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV6wuktxTHfX",
        "outputId": "2e90919d-61c7-45d4-80af-b4f796db3ed3"
      },
      "id": "wV6wuktxTHfX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ANSWER ===\n",
            "\n",
            "The indexed videos focus on job interview preparation and strategies. Here's an overview of the content:\n",
            "\n",
            "1. **Interview Preparation**: The videos emphasize the importance of being well-prepared for interviews, including having a list of thoughtful questions to ask the employer. Suggested questions include inquiries about the company culture, goals, and performance evaluation.\n",
            "\n",
            "2. **Positive Framing**: Candidates are advised to avoid speaking negatively about previous employers. Instead, they should highlight what they learned from past experiences and how those experiences can benefit the new role.\n",
            "\n",
            "3. **Body Language and Etiquette**: The videos stress the significance of body language during interviews. Candidates should maintain good posture, make eye contact, and be aware of their movements to convey confidence and presence.\n",
            "\n",
            "4. **Answering Common Questions**: Tips are provided on how to answer common interview questions effectively. Candidates are encouraged to be honest and reflect on their experiences in relation to the job they are applying for.\n",
            "\n",
            "5. **Follow-Up**: After the interview, it's crucial to send a thank-you email to the hiring manager within 24 hours. This email can be brief or more detailed, reinforcing the candidate's interest and leaving a positive impression.\n",
            "\n",
            "Overall, the videos aim to equip job seekers with practical advice and insights to enhance their interview performance and increase their chances of securing a job.\n",
            "\n",
            "=== CONTEXT USED ===\n",
            "\n",
            "[1] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 4:\n",
            "chatting so far. I guess my last question is, do\n",
            "you have any questions for me? Oh, this is a hot one. Make sure to have a short list\n",
            "of questions for your employer in your back pocket. Ask the questions you\n",
            "would if you got the job. If tomorrow was your\n",
            "first day what would you want to know\n",
            "from the manager. Even if you don't have any\n",
            "burning questions, asking a few shows that you did your homework\n",
            "and that you really care. Some include, what do you\n",
            "like best about working here? What are some mistakes people\n",
            "have made in this position? What is a goal you're\n",
            "currently working toward? How will my performance\n",
            "be evaluated? Well, I did read that you're\n",
            "expanding your software team next quarter. I'm curious how you\n",
            "plan to carry that out. The interview isn't\n",
            "actually over. Always follow up within 24\n",
            "hours with the thank you email to the hiring manager. This can be a quick note simply\n",
            "thanking them for their time. Or a longer one that elaborates\n",
            "\n",
            "---\n",
            "[2] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 3:\n",
            "the other month-- Ooh, just really quick. Don't speak negatively about\n",
            "previous places you've worked. Instead, talk about\n",
            "what you've learned. I helped lead a team of five\n",
            "to deliver a three-week sales project a few days\n",
            "ahead of schedule. I'd love to bring\n",
            "these things here. Perfect. A response like this not\n",
            "only answers the question but also shows off\n",
            "her personality. But remember, there's no\n",
            "one right way to interview and answer questions. Be yourself, and let\n",
            "your personality shine. Be aware of your movements.\n",
            "Practice polite, confident body language. Subtly miming your\n",
            "interviewers posture can actually create a\n",
            "sense of connection. Of all places,\n",
            "unnecessary movements are hard to ignore in an interview. Whether it's tapping your\n",
            "fingers or bouncing your leg, be aware and stay present. Hey, it's been great\n",
            "chatting so far. I guess my last question is, do\n",
            "you have any questions for me? Oh, this is a hot one. Make sure to have a short list\n",
            "\n",
            "---\n",
            "[3] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 0:\n",
            "Arguably, the most crucial\n",
            "part of the job search. An interview can make\n",
            "or break an opportunity. So to help you really prepare, we're going to dissect and\n",
            "analyze an entire interview from start to finish. I'll be sprinkling in a mix\n",
            "of tips about body language, etiquette, and how to\n",
            "answer common questions, like when exactly does\n",
            "the interview start? How do you deal with nerves? And how soon can you follow up? For years, athletes have used\n",
            "science and data analysis to improve. Now we are doing the same\n",
            "for job seekers everywhere. This is Job Science. Meet Anya, a\n",
            "recent grad majoring in business administration. She's interviewing for an\n",
            "entry level project management position. Note her posture. Head up, shoulders pulled back,\n",
            "no slouching, and no laid-backness. The interview begins the minute\n",
            "you walk into the building. Anya treats everyone in\n",
            "the office with respect while keeping eye contact. From security personnel\n",
            "to receptionists. Anyone you run\n",
            "\n",
            "---\n",
            "[4] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 2:\n",
            "the common ones like, why do you want to work here? What makes you unique? Let's see what our\n",
            "interviewer asks. So I want to hear more. Tell me a little\n",
            "about your experience and what you'd\n",
            "bring to this role? Pause. When this is asked,\n",
            "they're looking to learn what makes\n",
            "you stand out. Be honest with your answers. If that means having to pause\n",
            "and think for a second, that is alright. Think about your\n",
            "past experiences and how the role lines up\n",
            "with your future goals. It never hurts to be honest. Great question. Ever since I was\n",
            "young, I've always been the organized\n",
            "one of my family, whether it was helping my\n",
            "parents schedule vacations or color-coordinating my closet. Naturally, that lifestyle got\n",
            "me here, project management. I've been a people-person for\n",
            "as long as I can remember. Plain and simple. I love team-building\n",
            "and making sure everyone has a part. Just\n",
            "the other month-- Ooh, just really quick. Don't speak negatively about\n",
            "previous places you've worked. Instead, talk about\n",
            "\n",
            "---\n",
            "[5] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 5:\n",
            "hours with the thank you email to the hiring manager. This can be a quick note simply\n",
            "thanking them for their time. Or a longer one that elaborates\n",
            "on some of the things you talked about. It's key to leaving\n",
            "a lasting impression. This was the breakdown\n",
            "of an interview. Till next time. [MUSIC PLAYING]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat loop\n",
        "\n"
      ],
      "metadata": {
        "id": "RrNr8dr9u-Gu"
      },
      "id": "RrNr8dr9u-Gu"
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_youtube_bot(session_id: str = \"default\"):\n",
        "    \"\"\"\n",
        "    Simple text-based chat loop in Colab, powered by the tool-calling agent + memory.\n",
        "    \"\"\"\n",
        "    print(\"YouTube QA ChatBot (agent powered)\")\n",
        "    print(\"Ask me anything about the videos I have indexed.\")\n",
        "    print(\"Type 'exit' or 'quit' to end the chat.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "            print(\"Bot: Bye!\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        # ✅ Use the agent (tools + memory), not the bare RAG function\n",
        "        answer = agent_chat(user_input, session_id=session_id)\n",
        "        print(f\"Bot: {answer}\\n\")\n",
        "\n",
        "\n",
        "# Run this to start chatting:\n",
        "chat_with_youtube_bot()\n"
      ],
      "metadata": {
        "id": "rQ1KOGBAZDyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44f122f-256d-4d2d-fb75-577c3d1dded5"
      },
      "id": "rQ1KOGBAZDyd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YouTube QA ChatBot (agent powered)\n",
            "Ask me anything about the videos I have indexed.\n",
            "Type 'exit' or 'quit' to end the chat.\n",
            "\n",
            "You: what should I not do?\n",
            "Bot: In an interview, you should avoid speaking negatively about previous employers and instead focus on what you've learned. Be mindful of your body language; avoid distracting movements like tapping your fingers. Also, prepare a few questions for your employer to show your interest and preparation.\n",
            "\n",
            "You: exit\n",
            "Bot: Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speech recognition"
      ],
      "metadata": {
        "id": "GwioHuC-0Tja"
      },
      "id": "GwioHuC-0Tja"
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio_to_text(audio_path: str, model: str = \"gpt-4o-mini-transcribe\") -> str:\n",
        "    \"\"\"\n",
        "    Transcribe an audio file to text using OpenAI's speech recognition.\n",
        "    Adjust model name if needed depending on what your account supports.\n",
        "    \"\"\"\n",
        "    with open(audio_path, \"rb\") as audio_file:\n",
        "        transcription = openai_client.audio.transcriptions.create(\n",
        "            model=model,\n",
        "            file=audio_file,\n",
        "            response_format=\"text\",\n",
        "        )\n",
        "    # For newer clients this is already a string; if it's an object, cast to str\n",
        "    return str(transcription)\n",
        "\n",
        "def ask_bot_with_audio(session_id: str = \"voice-session\"):\n",
        "    \"\"\"\n",
        "    Colab helper to:\n",
        "    1. Upload an audio file with a spoken question\n",
        "    2. Use the agent (via answer_from_audio_file) to get transcript + answer\n",
        "    3. Print both transcript and answer\n",
        "    \"\"\"\n",
        "    print(\"🎙️ Upload an audio file (e.g., .wav, .mp3) with your question...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded.\")\n",
        "        return\n",
        "\n",
        "    audio_filename = next(iter(uploaded.keys()))\n",
        "    print(f\"📁 Uploaded file: {audio_filename}\")\n",
        "\n",
        "    try:\n",
        "        # This already uses agent_chat under the hood\n",
        "        transcript_text, answer = answer_from_audio_file(audio_filename, session_id=session_id)\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error while processing the audio:\", e)\n",
        "        return\n",
        "\n",
        "    print(\"\\n📝 Transcribed question:\")\n",
        "    print(transcript_text)\n",
        "\n",
        "    print(\"\\n🤖 Bot answer:\")\n",
        "    print(answer)\n",
        "\n",
        "    print(\"\\n Transcribed question:\")\n",
        "    print(transcript_text)\n",
        "\n",
        "    # 2) Ask the RAG bot\n",
        "    try:\n",
        "        answer, _context = youtube_rag_query(transcript_text, session_id=session_id)\n",
        "    except TypeError:\n",
        "        # Fallback if youtube_rag_query only expects (question)\n",
        "        answer, _context = youtube_rag_query(transcript_text)\n",
        "\n",
        "    print(\"\\n🤖 Bot answer:\")\n",
        "    print(answer)\n",
        "\n",
        "def tts_from_text(text: str, output_dir: str = \"tts_outputs\") -> str:\n",
        "    \"\"\"\n",
        "    Convert answer text to speech using OpenAI TTS and return the audio file path.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    filename = f\"answer_{uuid.uuid4().hex}.mp3\"\n",
        "    out_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Adjust 'model' if needed to whatever TTS model you have access to\n",
        "    speech = openai_client.audio.speech.create(\n",
        "        model=\"gpt-4o-mini-tts\",  # or another TTS-capable model from your OpenAI account\n",
        "        voice=\"alloy\",\n",
        "        input=text,\n",
        "    )\n",
        "\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        f.write(speech.read())\n",
        "\n",
        "    return out_path\n"
      ],
      "metadata": {
        "id": "3bP0GoOnvWVN"
      },
      "id": "3bP0GoOnvWVN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_bot_with_audio()"
      ],
      "metadata": {
        "id": "8v2gQx1Z3C6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "03e7c858-39bb-44cf-df14-1f8664ec7ac1"
      },
      "id": "8v2gQx1Z3C6c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎙️ Upload an audio file (e.g., .wav, .mp3) with your question...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9ba5887-2413-4dd2-a286-d022c42f2e11\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e9ba5887-2413-4dd2-a286-d022c42f2e11\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2172883381.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mask_bot_with_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2250406307.py\u001b[0m in \u001b[0;36mask_bot_with_audio\u001b[0;34m(session_id)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🎙️ Upload an audio file (e.g., .wav, .mp3) with your question...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_from_audio_file(audio_path: str, session_id: str = \"voice-session\"):\n",
        "    \"\"\"\n",
        "    Core backend logic:\n",
        "    1. Transcribe audio file at `audio_path` to text\n",
        "    2. Run the text through the agent (which internally uses RAG + tools + memory)\n",
        "    3. Return (transcript, answer)\n",
        "    \"\"\"\n",
        "    transcript_text = transcribe_audio_to_text(audio_path)\n",
        "    answer = agent_chat(transcript_text, session_id=session_id)\n",
        "    return transcript_text, answer\n"
      ],
      "metadata": {
        "id": "82koS--63Gkh"
      },
      "id": "82koS--63Gkh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change for Gradio later"
      ],
      "metadata": {
        "id": "LDoXdxTW97Sy"
      },
      "id": "LDoXdxTW97Sy"
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_audio_qa(audio_path, tts_enabled: bool):\n",
        "    \"\"\"\n",
        "    Gradio backend:\n",
        "    - Takes an audio question (mic or upload)\n",
        "    - Uses the agent-backed pipeline to get (transcript, answer)\n",
        "    - Optionally generates spoken answer audio if tts_enabled is True\n",
        "    \"\"\"\n",
        "    if audio_path is None:\n",
        "        return \"\", \"No audio received.\", None\n",
        "\n",
        "    # This already calls agent_chat under the hood\n",
        "    transcript, answer = answer_from_audio_file(audio_path)\n",
        "\n",
        "    answer_audio_path = None\n",
        "    if tts_enabled:\n",
        "        try:\n",
        "            answer_audio_path = tts_from_text(answer)\n",
        "        except Exception as e:\n",
        "            print(\"TTS error:\", e)\n",
        "\n",
        "    return transcript, answer, answer_audio_path\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🎬 YouTube QA Bot with Voice Input\")\n",
        "\n",
        "    with gr.Row():\n",
        "        audio_input = gr.Audio(\n",
        "            sources=[\"microphone\"],   # 👈 mic only\n",
        "            type=\"filepath\",          # pass a file path to backend\n",
        "            label=\"Hold to record your question\",\n",
        "        )\n",
        "        tts_toggle = gr.Checkbox(\n",
        "            label=\"Read answer aloud\",\n",
        "            value=False,\n",
        "        )\n",
        "\n",
        "    transcript_output = gr.Textbox(\n",
        "        label=\"Transcribed question\",\n",
        "        lines=2,\n",
        "    )\n",
        "    answer_output = gr.Textbox(\n",
        "        label=\"Bot answer\",\n",
        "        lines=4,\n",
        "    )\n",
        "    answer_audio_output = gr.Audio(\n",
        "        label=\"Spoken answer\",\n",
        "        type=\"filepath\",\n",
        "    )\n",
        "\n",
        "    audio_input.change(\n",
        "        fn=gradio_audio_qa,\n",
        "        inputs=[audio_input, tts_toggle],\n",
        "        outputs=[transcript_output, answer_output, answer_audio_output],\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "RD3CLyAp98Hr"
      },
      "id": "RD3CLyAp98Hr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "main_version4.ipynb",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}