{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kr5red/Project-4-Business-Case-Multimodal-AI-ChatBot-for-YouTube-Video-QA/blob/main/main_version2_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0886a0",
      "metadata": {
        "id": "6c0886a0"
      },
      "source": [
        "# YouTube RAG Pipeline\n",
        "\n",
        "0. Installments & Imports\n",
        "1. Ingest YouTube Videos → DataFrame\n",
        "2. Convert Transcripts → LangChain Documents\n",
        "3. Build Vector Store (Chroma + OpenAI Embeddings)\n",
        "4. (Next Steps – Implemented in Later Cells)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Installments & Imports"
      ],
      "metadata": {
        "id": "l2dqM0Be1PI-"
      },
      "id": "l2dqM0Be1PI-"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api chromadb pytube\n",
        "!pip install -q -U langchain langchain-openai langchain-core langchain-community langsmith"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLyu0wulxdWa",
        "outputId": "dd16d2cb-4826-49de-daf9-e7eee6bb66d1"
      },
      "id": "DLyu0wulxdWa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.12/dist-packages (15.0.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.9/410.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "from youtube_transcript_api import (\n",
        "    YouTubeTranscriptApi,\n",
        "    TranscriptsDisabled,\n",
        "    NoTranscriptFound,\n",
        ")\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "eXO5JhUm1egX"
      },
      "id": "eXO5JhUm1egX",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YouTube ingestion\n",
        "- URL -> video_id\n",
        "- video_id -> transcript (list)\n",
        "- transcript -> plain text"
      ],
      "metadata": {
        "id": "qFHwe7lD2TjT"
      },
      "id": "qFHwe7lD2TjT"
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the YouTube video ID from URL formats\n",
        "def extract_video_id(url: str) -> str:\n",
        "    parsed = urlparse(url)\n",
        "\n",
        "    # Short youtu.be links\n",
        "    if parsed.netloc in (\"youtu.be\", \"www.youtu.be\"):\n",
        "        return parsed.path.lstrip(\"/\")\n",
        "\n",
        "    # Regular youtube.com links\n",
        "    if parsed.netloc in (\"www.youtube.com\", \"youtube.com\", \"m.youtube.com\"):\n",
        "        qs = parse_qs(parsed.query)\n",
        "        vid = qs.get(\"v\", [None])[0]\n",
        "        if vid:\n",
        "            return vid\n",
        "\n",
        "    raise ValueError(f\"Could not extract video_id from URL: {url}\")\n",
        "\n",
        "#Convert a transcript (list of {text, start, duration}) to a single text string\n",
        "def transcript_to_text(transcript, include_timestamps: bool = False) -> str:\n",
        "    lines = []\n",
        "    for entry in transcript:\n",
        "        if include_timestamps:\n",
        "            start = entry[\"start\"]\n",
        "            lines.append(f\"[{start:.1f}s] {entry['text']}\")\n",
        "        else:\n",
        "            lines.append(entry[\"text\"])\n",
        "    return \" \".join(lines)\n",
        "\n",
        "\n",
        "#Fetch transcript for a single video_id and turn it into plain text.\n",
        "def fetch_transcript_text(video_id: str, languages=None) -> str:\n",
        "    try:\n",
        "        ytt_api = YouTubeTranscriptApi()\n",
        "\n",
        "        # If you don't care about language, you can call ytt_api.fetch(video_id) without languages\n",
        "        if languages is None:\n",
        "            fetched = ytt_api.fetch(video_id)\n",
        "        else:\n",
        "            fetched = ytt_api.fetch(video_id, languages=languages)\n",
        "\n",
        "        # `fetched` is a FetchedTranscript object with `.snippets`\n",
        "        # Convert to the same structure transcript_to_text() expects\n",
        "        transcript = [\n",
        "            {\"text\": s.text, \"start\": s.start, \"duration\": s.duration}\n",
        "            for s in fetched.snippets\n",
        "        ]\n",
        "\n",
        "        return transcript_to_text(transcript, include_timestamps=False)\n",
        "\n",
        "    except TranscriptsDisabled:\n",
        "        raise RuntimeError(f\"Transcripts are disabled for video_id={video_id}\")\n",
        "    except NoTranscriptFound:\n",
        "        raise RuntimeError(f\"No transcript found for video_id={video_id} in languages={languages}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error fetching transcript for {video_id}: {e}\")\n"
      ],
      "metadata": {
        "id": "Bs8Ik8rc2fwc"
      },
      "id": "Bs8Ik8rc2fwc",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ingest YouTube videos into a DataFrame"
      ],
      "metadata": {
        "id": "c_LlEnne4kxE"
      },
      "id": "c_LlEnne4kxE"
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_youtube_videos(urls, languages=\"en, de\") -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            video_id = extract_video_id(url)\n",
        "            transcript = fetch_transcript_text(video_id, languages=languages)\n",
        "            rows.append({\n",
        "                \"video_id\": video_id,\n",
        "                \"url\": url,\n",
        "                \"transcript\": transcript,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {url}: {e}\")\n",
        "    return pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "id": "5jefkcqOe_z4"
      },
      "id": "5jefkcqOe_z4",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ingest multiple videos ----\n",
        "video_urls = [\n",
        "    \"https://www.youtube.com/watch?v=HG68Ymazo18\",\n",
        "]\n",
        "\n",
        "df_videos = ingest_youtube_videos(video_urls, languages=[\"en\"])\n",
        "\n",
        "if df_videos.empty:\n",
        "    print(\"No videos ingested ...\")\n",
        "else:\n",
        "    print(df_videos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB4vj0G-6cAW",
        "outputId": "7fc38d40-9c59-4a9e-f8fb-4d1e347852c4"
      },
      "id": "TB4vj0G-6cAW",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      video_id                                          url  \\\n",
            "0  HG68Ymazo18  https://www.youtube.com/watch?v=HG68Ymazo18   \n",
            "\n",
            "                                          transcript  \n",
            "0  Arguably, the most crucial\\npart of the job se...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_videos.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "FV1jSMsL-bgG",
        "outputId": "3dca4436-0fff-47a1-e03c-489f297b4ad6"
      },
      "id": "FV1jSMsL-bgG",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      video_id                                          url  \\\n",
              "0  HG68Ymazo18  https://www.youtube.com/watch?v=HG68Ymazo18   \n",
              "\n",
              "                                          transcript  \n",
              "0  Arguably, the most crucial\\npart of the job se...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd0c3585-e6f3-4204-a0c6-d063662d5b42\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_id</th>\n",
              "      <th>url</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HG68Ymazo18</td>\n",
              "      <td>https://www.youtube.com/watch?v=HG68Ymazo18</td>\n",
              "      <td>Arguably, the most crucial\\npart of the job se...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd0c3585-e6f3-4204-a0c6-d063662d5b42')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fd0c3585-e6f3-4204-a0c6-d063662d5b42 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fd0c3585-e6f3-4204-a0c6-d063662d5b42');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_videos",
              "summary": "{\n  \"name\": \"df_videos\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"HG68Ymazo18\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://www.youtube.com/watch?v=HG68Ymazo18\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Arguably, the most crucial\\npart of the job search. An interview can make\\nor break an opportunity. So to help you really prepare, we're going to dissect and\\nanalyze an entire interview from start to finish. I'll be sprinkling in a mix\\nof tips about body language, etiquette, and how to\\nanswer common questions, like when exactly does\\nthe interview start? How do you deal with nerves? And how soon can you follow up? For years, athletes have used\\nscience and data analysis to improve. Now we are doing the same\\nfor job seekers everywhere. This is Job Science. Meet Anya, a\\nrecent grad majoring in business administration. She's interviewing for an\\nentry level project management position. Note her posture. Head up, shoulders pulled back,\\nno slouching, and no laid-backness. The interview begins the minute\\nyou walk into the building. Anya treats everyone in\\nthe office with respect while keeping eye contact. From security personnel\\nto receptionists. Anyone you run\\ninto on your way in could be asked to\\ngive feedback on you. It's normal to be nervous. When nerves kick in, the\\nnatural human response is to take short breaths\\nand breathe faster. Stay calm by taking a deep\\nbreath before entering. Hold it. Count to three and then\\nslowly breathe out. Pause. First, ace those introductions. Greet everyone in a way\\nthat is authentic to you, like, hey, nice to meet you. And then say their name. That one always works. You know you're more\\nlikely to remember their name if you say it out\\nloud when you first meet. A lot of the time, small talk\\ncomes up before any questions. It's good to have a few current\\nevents or topics in mind. You can't possibly predict\\nwhat they're going to ask. But you can practice answering\\nthe common ones like, why do you want to work here? What makes you unique? Let's see what our\\ninterviewer asks. So I want to hear more. Tell me a little\\nabout your experience and what you'd\\nbring to this role? Pause. When this is asked,\\nthey're looking to learn what makes\\nyou stand out. Be honest with your answers. If that means having to pause\\nand think for a second, that is alright. Think about your\\npast experiences and how the role lines up\\nwith your future goals. It never hurts to be honest. Great question. Ever since I was\\nyoung, I've always been the organized\\none of my family, whether it was helping my\\nparents schedule vacations or color-coordinating my closet. Naturally, that lifestyle got\\nme here, project management. I've been a people-person for\\nas long as I can remember. Plain and simple. I love team-building\\nand making sure everyone has a part. Just\\nthe other month-- Ooh, just really quick. Don't speak negatively about\\nprevious places you've worked. Instead, talk about\\nwhat you've learned. I helped lead a team of five\\nto deliver a three-week sales project a few days\\nahead of schedule. I'd love to bring\\nthese things here. Perfect. A response like this not\\nonly answers the question but also shows off\\nher personality. But remember, there's no\\none right way to interview and answer questions. Be yourself, and let\\nyour personality shine. Be aware of your movements.\\nPractice polite, confident body language. Subtly miming your\\ninterviewers posture can actually create a\\nsense of connection. Of all places,\\nunnecessary movements are hard to ignore in an interview. Whether it's tapping your\\nfingers or bouncing your leg, be aware and stay present. Hey, it's been great\\nchatting so far. I guess my last question is, do\\nyou have any questions for me? Oh, this is a hot one. Make sure to have a short list\\nof questions for your employer in your back pocket. Ask the questions you\\nwould if you got the job. If tomorrow was your\\nfirst day what would you want to know\\nfrom the manager. Even if you don't have any\\nburning questions, asking a few shows that you did your homework\\nand that you really care. Some include, what do you\\nlike best about working here? What are some mistakes people\\nhave made in this position? What is a goal you're\\ncurrently working toward? How will my performance\\nbe evaluated? Well, I did read that you're\\nexpanding your software team next quarter. I'm curious how you\\nplan to carry that out. The interview isn't\\nactually over. Always follow up within 24\\nhours with the thank you email to the hiring manager. This can be a quick note simply\\nthanking them for their time. Or a longer one that elaborates\\non some of the things you talked about. It's key to leaving\\na lasting impression. This was the breakdown\\nof an interview. Till next time. [MUSIC PLAYING]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Chunking + LangChain Documents on Top the Your DataFrame"
      ],
      "metadata": {
        "id": "6Cc_3H_tg-PY"
      },
      "id": "6Cc_3H_tg-PY"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "from pytube import YouTube  # to enrich metadata from YouTube\n",
        "\n",
        "#Convert each row in df_videos (video_id, url, transcript) into multiple LangChain Documents with metadata.\n",
        "def df_to_documents(\n",
        "    df: pd.DataFrame,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 150,\n",
        ") -> list[Document]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "\n",
        "    docs: list[Document] = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        video_id = row[\"video_id\"]\n",
        "        url = row[\"url\"]\n",
        "        transcript = row[\"transcript\"]\n",
        "\n",
        "        # Try to fetch some metadata from YouTube\n",
        "        title = author = description = None\n",
        "        try:\n",
        "            yt = YouTube(url)\n",
        "            title = yt.title\n",
        "            author = yt.author\n",
        "            description = yt.description\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Split transcript into chunks\n",
        "        chunks = splitter.split_text(transcript)\n",
        "\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            doc = Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"video_id\": video_id,\n",
        "                    \"url\": url,\n",
        "                    \"title\": title,\n",
        "                    \"author\": author,\n",
        "                    \"description\": description,\n",
        "                    \"chunk_index\": idx,\n",
        "                },\n",
        "            )\n",
        "            docs.append(doc)\n",
        "\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "DQAyQkdpg9LU"
      },
      "id": "DQAyQkdpg9LU",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df_to_documents(df_videos)\n",
        "print(f\"Created {len(documents)} chunks from {len(df_videos)} videos.\")"
      ],
      "metadata": {
        "id": "SRPjokEDhUVi",
        "outputId": "73302902-ac01-45c5-86ee-b7329ce5f9dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SRPjokEDhUVi",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 6 chunks from 1 videos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a LangChain VectorStore (Chroma) from Documents"
      ],
      "metadata": {
        "id": "ZAM2QMuAmBjJ"
      },
      "id": "ZAM2QMuAmBjJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata  # this is what reads Colab secrets\n",
        "\n",
        "# Get the key from Colab Secrets\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if openai_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in Colab secrets. Check the name.\")\n",
        "\n",
        "# Option A: set as environment variable so LangChain & others can use it\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "\n",
        "print(\"Loaded OPENAI_API_KEY from Colab secrets:\", os.environ[\"OPENAI_API_KEY\"] is not None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVxtKUobyQxd",
        "outputId": "aefb1c65-750c-4d43-f3d0-9f8830c3c51f"
      },
      "id": "yVxtKUobyQxd",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded OPENAI_API_KEY from Colab secrets: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings  # requires OPENAI_API_KEY\n",
        "\n",
        "# Or keep using SentenceTransformer embeddings if you prefer local:\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def build_vectorstore_from_documents(\n",
        "    docs: list[Document],\n",
        "    collection_name: str = \"youtube_rag\",\n",
        "    persist_directory: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a Chroma vector store from LangChain Documents.\n",
        "    Uses OpenAI embeddings by default.\n",
        "    \"\"\"\n",
        "    # OpenAI embedding model\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "        collection_name=collection_name,\n",
        "        persist_directory=persist_directory,  # can be None for in-memory\n",
        "    )\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "23CYzLAnl0ly"
      },
      "id": "23CYzLAnl0ly",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df_to_documents(df_videos)\n",
        "vectorstore = build_vectorstore_from_documents(\n",
        "    documents,\n",
        "    collection_name=\"youtube_rag\",\n",
        "    persist_directory=\"./chroma_youtube_rag\",\n",
        ")"
      ],
      "metadata": {
        "id": "TOeHGCPpmCkR"
      },
      "id": "TOeHGCPpmCkR",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangSmith & env configuration\n"
      ],
      "metadata": {
        "id": "wfhQH3S0Pso1"
      },
      "id": "wfhQH3S0Pso1"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "langchain_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"youtube-qa-bot\"\n",
        "\n",
        "print(\"LangSmith enabled for project:\", os.environ[\"LANGCHAIN_PROJECT\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5yoJLGTHZZ9",
        "outputId": "e10c915e-0065-4c6e-b3aa-0393d5329680"
      },
      "id": "V5yoJLGTHZZ9",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangSmith enabled for project: youtube-qa-bot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modern LangChain imports\n"
      ],
      "metadata": {
        "id": "XObj0HH8QSSp"
      },
      "id": "XObj0HH8QSSp"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory"
      ],
      "metadata": {
        "id": "UPCOn_-mQMoY"
      },
      "id": "UPCOn_-mQMoY",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM + Retriever + Memory"
      ],
      "metadata": {
        "id": "Od7TS69bS2ud"
      },
      "id": "Od7TS69bS2ud"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "memory = ChatMessageHistory()"
      ],
      "metadata": {
        "id": "Lx-_wSeDQaZG"
      },
      "id": "Lx-_wSeDQaZG",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the RAG pipeline manually"
      ],
      "metadata": {
        "id": "wXCjLu7RS9gP"
      },
      "id": "wXCjLu7RS9gP"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant answering questions about the content \"\n",
        "            \"of YouTube videos indexed in a vector database. \"\n",
        "            \"Use the retrieved context to answer accurately.\"\n",
        "        ),\n",
        "        (\"human\", \"Context from videos:\\n{context}\\n\\nQuestion: {question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def youtube_rag_query(question: str, session_id: str = \"default\"):\n",
        "    \"\"\"\n",
        "    Full RAG pipeline:\n",
        "    1. Retrieve relevant video chunks\n",
        "    2. Add them into the prompt\n",
        "    3. Call the LLM\n",
        "    4. Store chat history (memory)\n",
        "    \"\"\"\n",
        "    # ---- Retrieval ----\n",
        "    docs = retriever.invoke(question)\n",
        "\n",
        "    if not docs:\n",
        "        context = \"No relevant content found.\"\n",
        "    else:\n",
        "        context = \"\\n\\n---\\n\".join(\n",
        "            f\"[{i+1}] Title: {d.metadata.get('title','Unknown')}\\n\"\n",
        "            f\"URL: {d.metadata.get('url','Unknown')}\\n\"\n",
        "            f\"Snippet: {d.page_content[:350].replace('\\n',' ')}\"\n",
        "            for i, d in enumerate(docs)\n",
        "        )\n",
        "\n",
        "    # ---- Build prompt ----\n",
        "    prompt_msg = rag_prompt.format_messages(\n",
        "        context=context,\n",
        "        question=question,\n",
        "    )\n",
        "\n",
        "    # ---- LLM call ----\n",
        "    response = llm.invoke(prompt_msg)\n",
        "\n",
        "    # ---- Memory update ----\n",
        "    memory.add_message(HumanMessage(content=question))\n",
        "    memory.add_message(response)\n",
        "\n",
        "    return response.content, context"
      ],
      "metadata": {
        "id": "EB5XMUeFS5r9"
      },
      "id": "EB5XMUeFS5r9",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer, used_context = youtube_rag_query(\"Give me an overview of the videos you indexed.\")\n",
        "\n",
        "print(\"=== ANSWER ===\\n\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\n=== CONTEXT USED ===\\n\")\n",
        "print(used_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV6wuktxTHfX",
        "outputId": "715ffb41-78cd-4d0c-f5e6-2468bef4bab8"
      },
      "id": "wV6wuktxTHfX",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ANSWER ===\n",
            "\n",
            "The videos indexed appear to focus on interview preparation and strategies for job seekers. Here’s an overview based on the snippets:\n",
            "\n",
            "1. **Interview Preparation**: The videos emphasize the importance of being well-prepared for interviews, suggesting that candidates should have a list of questions ready to ask potential employers. This preparation can help candidates understand the role better and demonstrate their interest.\n",
            "\n",
            "2. **Positive Framing**: Candidates are advised to avoid speaking negatively about previous employers. Instead, they should focus on what they learned from past experiences and how those lessons can be applied to the new role.\n",
            "\n",
            "3. **Interview Dynamics**: The content discusses the structure of interviews, including common questions that candidates might face, such as \"Why do you want to work here?\" and \"What makes you unique?\" It highlights the importance of being honest and reflective in responses.\n",
            "\n",
            "4. **Body Language and Etiquette**: Tips on body language and etiquette during interviews are also provided, indicating that non-verbal communication plays a significant role in how candidates are perceived.\n",
            "\n",
            "5. **Follow-Up**: The videos conclude with advice on sending a thank-you email to the hiring manager after the interview, which is crucial for leaving a positive impression.\n",
            "\n",
            "Overall, the videos serve as a comprehensive guide for candidates to navigate the interview process effectively, from preparation to follow-up.\n",
            "\n",
            "=== CONTEXT USED ===\n",
            "\n",
            "[1] Title: Unknown\n",
            "URL: https://www.youtube.com/watch?v=HG68Ymazo18\n",
            "Snippet: chatting so far. I guess my last question is, do you have any questions for me? Oh, this is a hot one. Make sure to have a short list of questions for your employer in your back pocket. Ask the questions you would if you got the job. If tomorrow was your first day what would you want to know from the manager. Even if you don't have any burning ques\n",
            "\n",
            "---\n",
            "[2] Title: Unknown\n",
            "URL: https://www.youtube.com/watch?v=HG68Ymazo18\n",
            "Snippet: the other month-- Ooh, just really quick. Don't speak negatively about previous places you've worked. Instead, talk about what you've learned. I helped lead a team of five to deliver a three-week sales project a few days ahead of schedule. I'd love to bring these things here. Perfect. A response like this not only answers the question but also show\n",
            "\n",
            "---\n",
            "[3] Title: Unknown\n",
            "URL: https://www.youtube.com/watch?v=HG68Ymazo18\n",
            "Snippet: Arguably, the most crucial part of the job search. An interview can make or break an opportunity. So to help you really prepare, we're going to dissect and analyze an entire interview from start to finish. I'll be sprinkling in a mix of tips about body language, etiquette, and how to answer common questions, like when exactly does the interview sta\n",
            "\n",
            "---\n",
            "[4] Title: Unknown\n",
            "URL: https://www.youtube.com/watch?v=HG68Ymazo18\n",
            "Snippet: the common ones like, why do you want to work here? What makes you unique? Let's see what our interviewer asks. So I want to hear more. Tell me a little about your experience and what you'd bring to this role? Pause. When this is asked, they're looking to learn what makes you stand out. Be honest with your answers. If that means having to pause and\n",
            "\n",
            "---\n",
            "[5] Title: Unknown\n",
            "URL: https://www.youtube.com/watch?v=HG68Ymazo18\n",
            "Snippet: hours with the thank you email to the hiring manager. This can be a quick note simply thanking them for their time. Or a longer one that elaborates on some of the things you talked about. It's key to leaving a lasting impression. This was the breakdown of an interview. Till next time. [MUSIC PLAYING]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "main_version2.ipynb",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}