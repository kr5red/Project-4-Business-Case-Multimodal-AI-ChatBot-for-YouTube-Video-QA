{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6c0886a0",
      "metadata": {
        "id": "6c0886a0"
      },
      "source": [
        "# YouTube RAG Pipeline\n",
        "\n",
        "0. Installments & Imports\n",
        "1. Ingest YouTube Videos → DataFrame\n",
        "2. Convert Transcripts → LangChain Documents\n",
        "3. Build Vector Store (Chroma + OpenAI Embeddings)\n",
        "4. (Next Steps – Implemented in Later Cells)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Installments & Imports"
      ],
      "metadata": {
        "id": "l2dqM0Be1PI-"
      },
      "id": "l2dqM0Be1PI-"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api chromadb pytube\n",
        "!pip install -q -U langchain langchain-openai langchain-core langchain-community langsmith\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLyu0wulxdWa",
        "outputId": "94cd4036-67d3-4518-9652-a414ebbba03b"
      },
      "id": "DLyu0wulxdWa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.12/dist-packages (15.0.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "# Colab secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain imports (modern API)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import Tool, tool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langsmith import Client\n",
        "from langsmith.evaluation import evaluate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Text splitting + Documents\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# VectorDB\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# YouTube transcript\n",
        "from youtube_transcript_api import (\n",
        "    YouTubeTranscriptApi,\n",
        "    TranscriptsDisabled,\n",
        "    NoTranscriptFound,\n",
        ")\n",
        "\n",
        "# Metadata enrichment\n",
        "from pytube import YouTube\n",
        "\n",
        "#Speech recognition\n",
        "from google.colab import files\n",
        "from openai import OpenAI\n",
        "import uuid\n",
        "\n",
        "#Tools\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List\n",
        "\n",
        "#deployment\n",
        "import gradio as gr\n",
        "\n",
        "#evaluation\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu"
      ],
      "metadata": {
        "id": "eXO5JhUm1egX"
      },
      "id": "eXO5JhUm1egX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "langchain_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "if openai_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in Colab secrets.\")\n",
        "if langchain_key is None:\n",
        "    raise ValueError(\"LANGCHAIN_API_KEY not found in Colab secrets.\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"    # required for LangSmith\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"youtube-qa-bot\"\n",
        "\n",
        "# Reusable OpenAI client for audio transcription etc.\n",
        "openai_client = OpenAI()\n",
        "\n",
        "print(\"Keys loaded\")\n",
        "print(\"LangSmith enabled — project:\", os.environ[\"LANGCHAIN_PROJECT\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL_s4k7aZnDL",
        "outputId": "7d553cff-24a0-4ebc-83ba-484543b43b4c"
      },
      "id": "HL_s4k7aZnDL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys loaded\n",
            "LangSmith enabled — project: youtube-qa-bot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAI()\n",
        "print(\"OpenAI client initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygmPXjXsHNdA",
        "outputId": "58bcb0a6-e9a0-43a0-9901-7f8ff436e8ea"
      },
      "id": "ygmPXjXsHNdA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YouTube ingestion\n",
        "- URL -> video_id\n",
        "- video_id -> transcript (list)\n",
        "- transcript -> plain text"
      ],
      "metadata": {
        "id": "qFHwe7lD2TjT"
      },
      "id": "qFHwe7lD2TjT"
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the YouTube video ID from URL formats\n",
        "def extract_video_id(url: str) -> str:\n",
        "    parsed = urlparse(url)\n",
        "\n",
        "    # Short youtu.be links\n",
        "    if parsed.netloc in (\"youtu.be\", \"www.youtu.be\"):\n",
        "        return parsed.path.lstrip(\"/\")\n",
        "\n",
        "    # Regular youtube.com links\n",
        "    if parsed.netloc in (\"www.youtube.com\", \"youtube.com\", \"m.youtube.com\"):\n",
        "        qs = parse_qs(parsed.query)\n",
        "        vid = qs.get(\"v\", [None])[0]\n",
        "        if vid:\n",
        "            return vid\n",
        "\n",
        "    raise ValueError(f\"Could not extract video_id from URL: {url}\")\n",
        "\n",
        "#Convert a transcript (list of {text, start, duration}) to a single text string\n",
        "def transcript_to_text(transcript, include_timestamps: bool = False) -> str:\n",
        "    lines = []\n",
        "    for entry in transcript:\n",
        "        if include_timestamps:\n",
        "            start = entry[\"start\"]\n",
        "            lines.append(f\"[{start:.1f}s] {entry['text']}\")\n",
        "        else:\n",
        "            lines.append(entry[\"text\"])\n",
        "    return \" \".join(lines)\n",
        "\n",
        "\n",
        "#Fetch transcript for a single video_id and turn it into plain text.\n",
        "def fetch_transcript_text(video_id: str, languages=None) -> str:\n",
        "    try:\n",
        "        ytt_api = YouTubeTranscriptApi()\n",
        "\n",
        "        # If you don't care about language, you can call ytt_api.fetch(video_id) without languages\n",
        "        if languages is None:\n",
        "            fetched = ytt_api.fetch(video_id)\n",
        "        else:\n",
        "            fetched = ytt_api.fetch(video_id, languages=languages)\n",
        "\n",
        "        # `fetched` is a FetchedTranscript object with `.snippets`\n",
        "        # Convert to the same structure transcript_to_text() expects\n",
        "        transcript = [\n",
        "            {\"text\": s.text, \"start\": s.start, \"duration\": s.duration}\n",
        "            for s in fetched.snippets\n",
        "        ]\n",
        "\n",
        "        return transcript_to_text(transcript, include_timestamps=False)\n",
        "\n",
        "    except TranscriptsDisabled:\n",
        "        raise RuntimeError(f\"Transcripts are disabled for video_id={video_id}\")\n",
        "    except NoTranscriptFound:\n",
        "        raise RuntimeError(f\"No transcript found for video_id={video_id} in languages={languages}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error fetching transcript for {video_id}: {e}\")\n"
      ],
      "metadata": {
        "id": "Bs8Ik8rc2fwc"
      },
      "id": "Bs8Ik8rc2fwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ingest YouTube videos into a DataFrame"
      ],
      "metadata": {
        "id": "c_LlEnne4kxE"
      },
      "id": "c_LlEnne4kxE"
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_youtube_videos(urls, languages=None, save_dir=\"transcripts\"):\n",
        "    \"\"\"\n",
        "    Fetches YouTube transcripts and automatically saves each one as a .txt file.\n",
        "    If a transcript file already exists locally, it will be loaded instead of calling YouTube again.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    rows = []\n",
        "    for url in urls:\n",
        "        video_id = extract_video_id(url)\n",
        "        txt_path = os.path.join(save_dir, f\"{video_id}.txt\")\n",
        "        # Load local transcript if available\n",
        "        if os.path.exists(txt_path):\n",
        "            print(f\":starkes_häkchen: Loading saved transcript for {video_id}\")\n",
        "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                transcript = f.read()\n",
        "        else:\n",
        "            # Fetch from YouTube\n",
        "            try:\n",
        "                print(f\"↻ Fetching transcript from YouTube for {video_id}...\")\n",
        "                transcript = fetch_transcript_text(video_id, languages=languages)\n",
        "                # Save to .txt\n",
        "                with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(transcript)\n",
        "                print(f\":diskette: Transcript saved at {txt_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\":warnung: Skipping {url}: {e}\")\n",
        "                transcript = \"\"\n",
        "        rows.append({\n",
        "            \"video_id\": video_id,\n",
        "            \"url\": url,\n",
        "            \"transcript\": transcript,\n",
        "        })\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "s_3Ip6hqYjnV"
      },
      "id": "s_3Ip6hqYjnV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_local_transcripts(urls, save_dir=\"transcripts\"):\n",
        "    rows = []\n",
        "    for url in urls:\n",
        "        video_id = extract_video_id(url)\n",
        "        txt_path = os.path.join(save_dir, f\"{video_id}.txt\")\n",
        "        if not os.path.exists(txt_path):\n",
        "            print(f\":warnung: No local transcript found for {video_id}\")\n",
        "            transcript = \"\"\n",
        "        else:\n",
        "            print(f\":starkes_häkchen: Loading local transcript for {video_id}\")\n",
        "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                transcript = f.read()\n",
        "        rows.append({\n",
        "            \"video_id\": video_id,\n",
        "            \"url\": url,\n",
        "            \"transcript\": transcript,\n",
        "        })\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "8YzJC7gHY0uw"
      },
      "id": "8YzJC7gHY0uw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ingest multiple videos ----\n",
        "video_urls = [\n",
        "    \"https://www.youtube.com/watch?v=a43Je1KQY3s\", #\n",
        "    \"https://www.youtube.com/watch?v=ck5nw7R1uEs\", # FAANG Interview\n",
        "    \"https://www.youtube.com/watch?v=jXXOI01IuPs\", # What is happening in big tech interviews 2025\n",
        "    \"https://www.youtube.com/watch?v=YUL8ayPe1r8\", # Outdated resume experience\n",
        "    \"https://www.youtube.com/watch?v=pjqi_M3SPwY\", # Resume mistakes to avoid\n",
        "    \"https://www.youtube.com/watch?v=mmQcX6HpCGs\", # Interview\n",
        "    \"https://www.youtube.com/watch?v=dG3TdJn7JP4\", # Interview\n",
        "    \"https://www.youtube.com/watch?v=WdyiUe7_3cA\", # Interview\n",
        "    \"https://www.youtube.com/watch?v=m-pjMa43tho\", # Interview\n",
        "    \"https://www.youtube.com/watch?v=8OOvvJ0hd3M\"\n",
        "]\n",
        "\n",
        "df_videos = ingest_youtube_videos(video_urls, languages=[\"en\", \"de\"], save_dir=\"transcripts\"\n",
        ")\n",
        "\n",
        "if df_videos.empty:\n",
        "    print(\"No videos ingested ...\")\n",
        "else:\n",
        "    print(df_videos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB4vj0G-6cAW",
        "outputId": "b6f5354c-3bf6-44b7-e469-0f48fd66128a"
      },
      "id": "TB4vj0G-6cAW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "↻ Fetching transcript from YouTube for a43Je1KQY3s...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=a43Je1KQY3s: Error fetching transcript for a43Je1KQY3s: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=a43Je1KQY3s! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for ck5nw7R1uEs...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=ck5nw7R1uEs: Error fetching transcript for ck5nw7R1uEs: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=ck5nw7R1uEs! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for jXXOI01IuPs...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=jXXOI01IuPs: Error fetching transcript for jXXOI01IuPs: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=jXXOI01IuPs! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for YUL8ayPe1r8...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=YUL8ayPe1r8: Error fetching transcript for YUL8ayPe1r8: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=YUL8ayPe1r8! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for pjqi_M3SPwY...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=pjqi_M3SPwY: Error fetching transcript for pjqi_M3SPwY: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=pjqi_M3SPwY! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for mmQcX6HpCGs...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=mmQcX6HpCGs: Error fetching transcript for mmQcX6HpCGs: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=mmQcX6HpCGs! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for dG3TdJn7JP4...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=dG3TdJn7JP4: Error fetching transcript for dG3TdJn7JP4: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=dG3TdJn7JP4! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for WdyiUe7_3cA...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=WdyiUe7_3cA: Error fetching transcript for WdyiUe7_3cA: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=WdyiUe7_3cA! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for m-pjMa43tho...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=m-pjMa43tho: Error fetching transcript for m-pjMa43tho: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=m-pjMa43tho! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "↻ Fetching transcript from YouTube for 8OOvvJ0hd3M...\n",
            ":warnung: Skipping https://www.youtube.com/watch?v=8OOvvJ0hd3M: Error fetching transcript for 8OOvvJ0hd3M: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=8OOvvJ0hd3M! This is most likely caused by:\n",
            "\n",
            "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
            "- You have done too many requests and your IP has been blocked by YouTube\n",
            "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
            "\n",
            "There are two things you can do to work around this:\n",
            "1. Use proxies to hide your IP address, as explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
            "2. (NOT RECOMMENDED) If you authenticate your requests using cookies, you will be able to continue doing requests for a while. However, YouTube will eventually permanently ban the account that you have used to authenticate with! So only do this if you don't mind your account being banned!\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "      video_id                                          url transcript\n",
            "0  a43Je1KQY3s  https://www.youtube.com/watch?v=a43Je1KQY3s           \n",
            "1  ck5nw7R1uEs  https://www.youtube.com/watch?v=ck5nw7R1uEs           \n",
            "2  jXXOI01IuPs  https://www.youtube.com/watch?v=jXXOI01IuPs           \n",
            "3  YUL8ayPe1r8  https://www.youtube.com/watch?v=YUL8ayPe1r8           \n",
            "4  pjqi_M3SPwY  https://www.youtube.com/watch?v=pjqi_M3SPwY           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading saved transcription as txt. files"
      ],
      "metadata": {
        "id": "k3suQ9L-a1ib"
      },
      "id": "k3suQ9L-a1ib"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Open a file upload dialog (you can select all 10 .txt files at once)\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Read each uploaded file and store its contents in a dictionary\n",
        "texts = {}\n",
        "\n",
        "for filename, raw_data in uploaded.items():\n",
        "    # raw_data is binary → decode to UTF-8 text\n",
        "    texts[filename] = raw_data.decode('utf-8')\n",
        "\n",
        "# 3. Print a list of successfully loaded files\n",
        "print(\"Loaded files:\")\n",
        "for name in texts:\n",
        "    print(\" -\", name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "SZGh5VFOazJz",
        "outputId": "ff61352f-063a-4cb3-9447-6ca2cc3029e8"
      },
      "id": "SZGh5VFOazJz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-64bc2bb1-588f-43c7-b484-fec3be969268\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-64bc2bb1-588f-43c7-b484-fec3be969268\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pjqi_M3SPwY.txt to pjqi_M3SPwY (1).txt\n",
            "Saving mmQcX6HpCGs.txt to mmQcX6HpCGs (1).txt\n",
            "Saving m-pjMa43tho.txt to m-pjMa43tho (1).txt\n",
            "Saving jXXOI01IuPs.txt to jXXOI01IuPs (1).txt\n",
            "Saving dG3TdJn7JP4.txt to dG3TdJn7JP4 (1).txt\n",
            "Saving ck5nw7R1uEs.txt to ck5nw7R1uEs (1).txt\n",
            "Saving a43Je1KQY3s.txt to a43Je1KQY3s (1).txt\n",
            "Saving YUL8ayPe1r8.txt to YUL8ayPe1r8 (1).txt\n",
            "Saving WdyiUe7_3cA.txt to WdyiUe7_3cA (1).txt\n",
            "Saving 8OOvvJ0hd3M.txt to 8OOvvJ0hd3M (1).txt\n",
            "Loaded files:\n",
            " - pjqi_M3SPwY (1).txt\n",
            " - mmQcX6HpCGs (1).txt\n",
            " - m-pjMa43tho (1).txt\n",
            " - jXXOI01IuPs (1).txt\n",
            " - dG3TdJn7JP4 (1).txt\n",
            " - ck5nw7R1uEs (1).txt\n",
            " - a43Je1KQY3s (1).txt\n",
            " - YUL8ayPe1r8 (1).txt\n",
            " - WdyiUe7_3cA (1).txt\n",
            " - 8OOvvJ0hd3M (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 1. Map: video_id -> transcript text (clean filename: remove \".txt\" and \" (1)\" etc.)\n",
        "id_to_text = {}\n",
        "\n",
        "for filename, content in texts.items():\n",
        "    # remove .txt\n",
        "    base = filename[:-4] if filename.endswith(\".txt\") else filename\n",
        "    # remove a suffix like \" (1)\", \" (2)\", etc.\n",
        "    base = re.sub(r\" \\(\\d+\\)$\", \"\", base)\n",
        "    video_id = base\n",
        "    id_to_text[video_id] = content\n",
        "\n",
        "print(\"Cleaned video_ids from filenames:\")\n",
        "for vid in id_to_text.keys():\n",
        "    print(\" -\", vid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40yYojboiSeJ",
        "outputId": "4d13621c-777b-4409-f3c0-a661595327ab"
      },
      "id": "40yYojboiSeJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned video_ids from filenames:\n",
            " - pjqi_M3SPwY\n",
            " - mmQcX6HpCGs\n",
            " - m-pjMa43tho\n",
            " - jXXOI01IuPs\n",
            " - dG3TdJn7JP4\n",
            " - ck5nw7R1uEs\n",
            " - a43Je1KQY3s\n",
            " - YUL8ayPe1r8\n",
            " - WdyiUe7_3cA\n",
            " - 8OOvvJ0hd3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for url in video_urls:\n",
        "    video_id = extract_video_id(url)\n",
        "    transcript = id_to_text.get(video_id, \"\")\n",
        "    if not transcript:\n",
        "        print(f\"⚠️ Warning: no transcript found for video_id={video_id}\")\n",
        "    rows.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"url\": url,\n",
        "        \"transcript\": transcript,\n",
        "    })\n",
        "\n",
        "df_videos = pd.DataFrame(rows)\n",
        "df_videos.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zvVBZ2QCie_J",
        "outputId": "017c3027-9eb9-4619-9153-81aa926e7be1"
      },
      "id": "zvVBZ2QCie_J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      video_id                                          url  \\\n",
              "0  a43Je1KQY3s  https://www.youtube.com/watch?v=a43Je1KQY3s   \n",
              "1  ck5nw7R1uEs  https://www.youtube.com/watch?v=ck5nw7R1uEs   \n",
              "2  jXXOI01IuPs  https://www.youtube.com/watch?v=jXXOI01IuPs   \n",
              "3  YUL8ayPe1r8  https://www.youtube.com/watch?v=YUL8ayPe1r8   \n",
              "4  pjqi_M3SPwY  https://www.youtube.com/watch?v=pjqi_M3SPwY   \n",
              "\n",
              "                                          transcript  \n",
              "0  hey does your resume feel outdated or invisibl...  \n",
              "1  3 million that is the number of rums Google re...  \n",
              "2  Something strange is happening with tech inter...  \n",
              "3  Your resume isn't working because you're follo...  \n",
              "4  all right so when I was applying to my first f...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-305e2965-9e55-44ad-9e40-ae3299ec1683\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_id</th>\n",
              "      <th>url</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a43Je1KQY3s</td>\n",
              "      <td>https://www.youtube.com/watch?v=a43Je1KQY3s</td>\n",
              "      <td>hey does your resume feel outdated or invisibl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ck5nw7R1uEs</td>\n",
              "      <td>https://www.youtube.com/watch?v=ck5nw7R1uEs</td>\n",
              "      <td>3 million that is the number of rums Google re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jXXOI01IuPs</td>\n",
              "      <td>https://www.youtube.com/watch?v=jXXOI01IuPs</td>\n",
              "      <td>Something strange is happening with tech inter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>YUL8ayPe1r8</td>\n",
              "      <td>https://www.youtube.com/watch?v=YUL8ayPe1r8</td>\n",
              "      <td>Your resume isn't working because you're follo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pjqi_M3SPwY</td>\n",
              "      <td>https://www.youtube.com/watch?v=pjqi_M3SPwY</td>\n",
              "      <td>all right so when I was applying to my first f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-305e2965-9e55-44ad-9e40-ae3299ec1683')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-305e2965-9e55-44ad-9e40-ae3299ec1683 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-305e2965-9e55-44ad-9e40-ae3299ec1683');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-42b08056-fcf6-4554-acb3-31cb9dd294d3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42b08056-fcf6-4554-acb3-31cb9dd294d3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-42b08056-fcf6-4554-acb3-31cb9dd294d3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_videos",
              "summary": "{\n  \"name\": \"df_videos\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"video_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"m-pjMa43tho\",\n          \"ck5nw7R1uEs\",\n          \"mmQcX6HpCGs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"https://www.youtube.com/watch?v=m-pjMa43tho\",\n          \"https://www.youtube.com/watch?v=ck5nw7R1uEs\",\n          \"https://www.youtube.com/watch?v=mmQcX6HpCGs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"honestly getting an interview is hard enough that we do not need to Fumble the ball once we get in there so let's talk about the 12 things you never want to say in an interview but before we get into it if we haven't met my name is Cassandra I'm a career strategist and speaker and I want to talk about all things with your career whether that's job search to promotion and just enjoying the work we do because that's where we spend a lot of our lives so let's talk about those 12 things number 12. please don't ask at the end what would I do in this role huh did you not read the job description did you not do your due diligence now this one and a couple of the others I'm going to talk about it really comes down to the way you're asking something so you can ask about the role in an interview but I wouldn't ask it in terms of like well what would I do in this role I would say something like you know I read the job description and I talked to so and so in the department but I would love to hear in your words what you see the day-to-day of this role looking like right that's an informed question instead of just like so what do I do in this that makes people wonder like do you know what you're even interviewing for how can you tell me you really want this job you know or you're really passionate about it when you don't even know what the role is this next one is honestly one of my personal favorites number 11. oh I can handle any situation this one also gets used a lot in cover letters people be like and I can handle any situation really you can handle any situation so if tomorrow I said I'm bringing in my two pet lions and you need to tame them can you do that like you can't you can be calm Under Pressure but you can't handle any situation let's not put that out there I've worked with some pretty difficult people in my work experience before there are some weird requests out there there are some irrational requests out there don't say you can handle any situation because it's just not true okay number 10 and this is the controversial one let me clarify this because the last time I made a video like this four years ago five years ago people really came for me for it and it's how much does this pay or what's the expected salary now there is a time and place to ask this question I think you can ask on a phone screen about the pay I think you can ask at the end of an interview about it if it hasn't been brought up to you but again it's the way you say it and also I will be honest I think some of it is industry related there are some Industries where it's a super transparent conversation at the very beginning of the interview process and there are other Industries where they are not going to bring it up and they're going to let you go all the way through so if this is something that's really important for you instead of saying well how much would this pay or what would my salary be say something like you know I know if we proceed with this interview process this is this can be a long process and I would hate to waste anyone's time so would you be able to give me a bit of the salary range before we even proceed right again that's that's showing hey I'm trying to care about you it's not just about me here I'm trying to care about you too let's not waste anyone's time let's get down to business like let's let's learn this now before we move forward right this is why people sometimes ask on a first date like do you want to have kids because they desperately want to have kids and if the other person doesn't well then we shouldn't do date too like let's get it done now so you can ask about salary it's just how you ask about salary and really thinking through the appropriate time to do it the number nine mistake bashing a bad boss a bad previous boss okay y'all I've had some toxic bosses I've had some great bosses too I know none of my former employers watch these videos yet I always get scared about it that they're gonna be like she talking about me does she think we were all toxic no but I've had a couple of toxic bosses and the interview is not the place to bash them again I'm gonna equate it to dating if you go on a first date with someone who goes on and on essentially about their ex and how terrible they were you don't really know this person on the first date yet are you really judging their ex or are you judging them going they seem to be real hung up on their ex wonder what part they played in this or hmm that doesn't sound as bad as I thought or why are they still talking about that right it's the same thing in an interview you don't want to sit there harping on the bad boss you can in a sophisticated way talk about situations that maybe you didn't see eye to eye with your boss or why you are not the right fit for that situation and why this is a better fit but we don't go on and on about how toxic our boss is because even if it is a thousand percent true they don't know you enough yet to trust your judgment of your old boss all right the next one you want to really avoid sounding cocky in an interview now this is a very fine line confidence is needed in an interview right we want somebody who's confident I think a lot of times confidence actually shows through our passion for the role kind of the way we lean forward and smile as we're talking about things and then the way we clearly communicate what we do that's confidence cockiness is when we turn subtly into like here's what I did and then I did this other thing and yeah the team was terrible but I did this it's it's a very subtle difference because you are spending time what like 30 minutes to an hour and there could be multiple interviews talking about yourself you're spending all this time talking about yourself but it's the way you talk about yourself that matters so one thing that can really help in seeing where you are on this balance and I fully admit I've accidentally swung into Cockiness before I've had a coach clients where they've swung into Cockiness before the way to get past it is practice practice these answers with a friend and ask them did I sound confident or did I sound cocky have them help you kind of mold that into a confidence statement and not a cocky statement and if you want more help on this I do interview coaching you can find my link below we can talk all about it okay this next one is another like it's in how you phrase it you don't want to sound for the job you don't want to say something like I really need this job now this is one of those where people want to turn me off and be like who's dumb enough to say that in an interview but there are some of us out there come on friend you and me there are some of us out there who have let nerves take over or we are so desperate for the job that things have come out of our mouths that we know I shouldn't be saying this right now okay I have not actually said this one but there are other things that I've said in interviews that I am saying them my brain is like what are you doing why are you saying this this is not good take it back take it back so while it might seem crazy that someone say someone would say I really need this job people get nervous and they end up saying things they shouldn't so when they say why are you interested in this role or why should we hire you I really need this job you know I'm two weeks away from not being able to pay my mortgage my kids school bill is do like we don't say any of that and I have heard people bring in personal reasons like that of why they need the job and why they're interested in it and so you just kind of want to keep all of that out of it there's a difference between saying I really need this job and sounding passionate and enthusiastic and wanting the job right desperately needing wanting two different things so you want to focus more on saying things like I'm really excited for this opportunity because and talking about why you're excited for it what about the role excites you other than the paycheck oh the next one you don't need to say you're nervous okay they know you're I sound really aggressive when I did that sorry but like they know you're nervous and that's okay I actually think it's healthy if a little bit of nerves show through it shows that you care if you walked in this again is where we can border on cocky when you're leaning back in your seat and you're like yeah let's talk yeah here's why I think gonna be good for it you're a little too chill can come off cocky when you're nervous it it shows that you care about this that you're like I really want to do well so it's okay for some nerves to show through but we don't need to State them they understand that and if they are a good interviewer they are going to try and put you at ease not everyone is a good interviewer I know but don't worry if you're nervous don't get in your head being like Oh my gosh I'm nervous and they can see I'm nervous I look so nervous right now don't worry about it don't worry about it it'll be okay but you don't need a state for the record like sorry I'm just really nervous it's okay the next one is one that really bothers me in the room this is one that I'll be really honest when you hear it you might be like I don't get what's wrong with it but if you've ever been in the room when someone's done it it's real Annoying do not name drop that someone recommended you for the position it's it comes across like you think because this person encouraged you to apply or put in a referral for you that you're owed something it's very weird it's one of those that if you haven't experienced it it's sort of hard to imagine but I'm just letting you know now because I totally get from the interviewee perspective from the candidate perspective you're sitting there thinking oh I need to tell them that so and so said I'd be great for this role that is not gonna go over great in the room what you can do is if they ask you how did you prepare for this interview that is a question some people ask you can talk about how you talk to certain people and name certain people and what they told you about the role and how that made you excited to apply or excited to interview or how that informed you but I have seen multiple candidates before name drop in the room like yeah and so and so and so and so like the the high hiring like the directors of the department they were like and these two told me to apply as they're in a whole room of people including those two people and you could literally watch the whole room tense up and be like oh really like so what so we don't get a say it just is it doesn't work well you might just have to trust me on this one but trust me you don't want a name drop in the room okay the next one is going to be one of those again where you're like who would ever say that I've watched people say some real dumb things again no like when we are nervous we say dumb things if you have never used their product or service if you have never used their software or watched their show or read their blog or whatever don't talk about it don't say oh I never use blah blah blah or uh oh I haven't logged on to your software before like no you should have at least tried these things before you did it working in entertainment you know if I was interviewing for a TV show that I'd never watched before and those interviews come extremely last minute like hey can you come see us tomorrow I would get home from work and I would watch an hour of that show just so I had some understanding of what it's about right so even if you're not working in television there is a version of this and even if it's not for that specific product you need to think about the industry too so I had people in television script writing classes that would be like oh I never watch TV why are you here like if you don't even watch TV you don't have a passion for the industry why are you trying to learn how to write a TV script that you think other people should read so if you are looking at technology but you're not like keeping up on the latest tech info or you really want to work for a company with a green environmental Mission and you don't really care about the environment don't share that in the interview we gotta like tell them how we care about the same things okay so just use products say you're the same as the mission or just keep your mouth shut like we we need to either go along with the interest or just not say anything at all now in a similar vein this one happens a lot and happens by accident you will be asked something about like have you used our product or what's a favorite one of our product or what's your favorite feature I know Amazon does like a what's your favorite Amazon product or what's your favorite Google Google will do what's your favorite Google feature or product things like that so it doesn't have to be like a con a physical consumer good product or again my intern students I know when they'd interview for certain networks for television they would be asked what's one of your favorite Viacom shows right and then they would say oh I love the Real Housewives well Real Housewives is Bravo and that's owned by NBC Universal it's not owned by Viacom so know who you're talking to and make sure you are familiar with their products so that you can say I like this or this is the feature I like and make sure it's theirs the point of this tip is make sure you're talking about something that's theirs you don't want to interview for Del Taco like Del Taco corporate and then be like what's your favorite food and you're like oh the crunchy gordita and they're like that's Taco Bell they're down the street right so make sure you are talking about the right products and features for the right company it sounds simple but again in the midst of nerves we can lose this one okay I cheated a little bit and this next one is it's not a thing to never say but it's a thing to never do don't leave your phone on during an interview don't just put it to silent the same could be said if it's an if it's an online or a virtual interview same could be said with notifications on your computer turn off your phone turn off notifications because even I have been in so many interviews where someone's phone just started vibrating right like maybe their phone didn't go off maybe they didn't have to say oh sorry about that that would be embarrassing but it kept vibrating every time they got a text message or a ping or somebody just kept trying to call them and we could all hear it and it's super distracting and it can also depending on the person that you're interviewing with and I'll even say depending on their age it could really irritate them and make them think like oh they're not taking this seriously so just turn your phone off on your computer turn all notifications off close all programs everything now ding ding dings for everything so turn it all off and then the last one when they say hey those are all of our questions do you have any questions for us the answer is always yes never say no I'm good I don't think I have any questions thank you for answering everything like no no no you have questions so having questions at the end is showing them that you are genuinely interested in this position and you want to learn more about it more the company more about the role more about the team Etc so you always have questions at the end now I have made a whole slew of videos of questions you can ask at the end of an interview you can start with this one right here and if you need more help I have a free interview prep checklist you can get that in the description box below and I'll see you in the next one bye\",\n          \"3 million that is the number of rums Google receives every year and of those applicants they only hire around 0.2% in this video I'm going to show you how to write a resume that's going to grab any recruiters attention regardless of your professional background this is a Formula that I use to help my clients go from constant rejection to Landing interviews with their dream companies I've spent the last decade working full-time as a technical recruiter and Technical recruiting leader for companies like Google Lyft Uber Tik Tok and the New York Times I reviewed hundreds of thousands of resum\\u00e9s hired hundreds of people and helped many job Seekers succeed so for example my client Cameron was really unhappy in his job didn't even want to go to work wasn't able to get any traction when he would apply to jobs didn't know what to do and after working with me we were able to get him 10 job interviews and he got a job offer in 2 weeks then you have my other client Ravi who was really stuck in his dead end job and really had some long-term goals that he wanted to to get to so after working with me we were able to get him his next job with a $100,000 pay increase and he and his wife were able to pay off their student loan debt so in this video I'm going to break down the formula that I use on my clients to help them revamp their resumes and of course stay until the end cuz then I'm going to give you the good stuff the secret SAU keep watching section one preparation before you start writing your resume it is really important that you do your research and study the market the the reason why is because if you don't do that you're going to fall into this trap that most people fall into which is creating a generic resume in today's job market it is more important than ever to create a resume that is tailored to the job family or job vertical that you're wanting to apply for when you have a generic resume it really makes it difficult for recruiters to really understand who you are and the value that you actually bring to the job and the company that you're applying to so in this first step when you study the Market you're going to make a list first you're going to make a list of the companies that you want to work for then you're going to make a list of the jobs that you want to apply for then you're going to head over to those companies pages and you're going to look at LinkedIn and go scrape those job boards and see what jobs are posted when you go to LinkedIn and you look for jobs you may actually find companies that you weren't really interested in the past or you hadn't thought of but they have a job opening that's really enticing when you do this research what you're going to notice are patterns you're going to find these through points between each of these roles now once you find those through points then you're going to be able to craft a resume that's going to be tailored to those jobs that you want to apply for that way when you do write your resume and you need to tailor it more specifically to the jobs that you're applying for it's only going to take a couple of minutes now once you've completed this step then we can go to the next step which is to build your resume section two Market your personal brand your resume is a document that markets you to potential employers based on your past experience and and achievements it needs to reflect who you are as a business professional so let's talk about the summary section on the rese now I'll be really honest as a recruiter I don't really read summaries I go straight down to the experience section to determine if your experience aligns to the job that you're applying for however if you're in a situation that requires a little bit of extra explanation so for example if you were laid off from your last job you can actually mention that in the summary if you want to relocate you can mention that in the summary you can say open to relocation if you are making a career transition this summary will allow you the opportunity to explain how your skill set aligns to this job that you're applying for so here are the steps to crafting your professional summary who you are professionally begin with a clear statement of your professional capacities for example you can say experienced marketing manager with a proven track record in digital marketing and brand management value proposition and evidence so you're going to follow this with a value proposition that includes evidence so mention specific achievements like successfully let a campaign that boosted social media engagement by 40% you're going to provide evidence early on to substantiate your claims and make your summary much more credible supporting trade or memorable item add a trade or memorable item that makes you stand out like an award or a unique project for example you can say recipient of the 2022 marketing Excellence award for Innovative campaign strategies again avoid generic language like results driven professional or good communicator all of that makes your summary sound super boring and then career goals clearly State why you're applying for the position so for example you can say aiming to apply my expertise in digital marketing to help a dynamic company Drive Innovation and achieve its growth objectives again identify what's going to make you stand out and make you unique and separate yourself from all of the other applicants this could be specific skills or experiences or accomplishments section three achievements highlighting hiring teams want to know what you've accomplished and what business value you're going to bring to that business and the job you're applying for think about Sarah a sales manager initially her resume list of responsibilities like manage sales team and develop sales strategies while these points were true they didn't effectively showcase her business impact now after focusing on her achievements her bullet points read like this increased Regional sales by 25% in one year by implementing new training programs and sales strategies also use numbers whenever possible use percentages or dollar amounts talk about what you increased or decreased so for example you can say increase sales by 20% over a 6-month period which is much more impactful than just saying increase sales now when you're going to describe your achievements you can use a claim evidence approach so what you're going to do is you're going to start with a claim so for example you can say for your claim exceeded annual sales quotas then here's the evidence you can write achieve $22 million against a goal of 20 million in 2020 $25 million against a goal of $23 million in 2021 and on track to hit $32 million against a goal of $30 million in 2022 again what you want to do is you want to highlight the impact in the results of your work companies want to know did you save the company money did you increase profits were you able to increase productivity what were the results of the work that you've done section four skill section this is where a lot of people really mess up because what they end up doing is they end up writing a lot of generic terms like good communicator now you don't need to list every single skill that you have instead list the skills that you have that are relevant to the position that you're applying to so for example if you use specific tools let's say Google analytics or Salesforce CRM make sure you list those as well now depending on the job you're applying for when you're reading that job description see if they make mention of your proficiency level in those tools so for example let's say you're applying for a role as a senior business data analyst and Excel is something that you need to have for your job what you can write in your skills section is advanced in Excel section five professional experience now this is my favorite part of a resume because this is where you get to really showcase and highlight who you are and the impact that you've had on a business business what you want to do is in your experience section you want to write out bullet points because it's a lot easier for the person reading your resume to be able to read it an easy way to write your results on your resume is to use the Google XYZ formula the Google XYZ formula States accomplished X as measured by y by doing Z and here's an example you can say increased Regional sales by 25% by implementing a new training program and sales strategies resulting in a $1.2 million Revenue increase and again what you want to do is when you've done your research and you've read the job description align what you've read in the job description to what it is that they're looking for in the experience that you've had and so that way you can craft your Google XYZ formulas to align with the jobs and your experience so here's a before and after transformation so you can see another example so before responsible for overseeing marketing campaigns after directed a marketing campaign that increased website traffic by 30% within 3 months by implementing targeted SEO strategies section six education education is where you're going to list any education that you have outside of any type of certifications when you list out your education you want to name the institution that you went to the degree that you received and any relevant coursework that you took that is relevant to the job that you're applying for also if you are a newer graduate meaning you've graduated in the past 3 years what you want to do is you want to include your graduation dates and on your resume move move your education to the very top of the resume underneath your contact information the reason why is because that will easily indicate to the recruiter that you're a newer graduate if you're beyond 3 years then what you do is move your education section to after your experience section and remove your dates of graduation here's an example of how you can write your education on your resume University of California Berkeley Bachelor of Arts and marketing graduated 2022 honors Magnum Kare relevant coursework digital marketing consumer Behavior market research now if you do have any certifications you can either create a new section titled certifications and you can include it there or if you want to put it into your education section let's say cuz do formatting wise you want to make sure it fits then at the top of the section just say education and certifications then you're going to list out all of your certifications and if any of those certifications have any type of IDs go ahead and leave those there and if there're any external links to those certifications so that the recruiter can click on it then include those as well section seven final touches formatting again another favorite I feel like every section is my favorite but no but really it's my favorite now the key to formatting is you want it to be simple and easy to read and make sure it's easily scannable now I know that there are websites like canva and on Etsy where you can get resum\\u00e9 templates even though canva offers r\\u00e9 templates I do not not recommend them yes they are beautiful and by the way I subscribe to canva I think it's an awesome tool I use it for so many things but when it comes to resum\\u00e9 templates I do not recommend it here's why most of the time when you are downloading these resum\\u00e9 templates a lot of them have clip art and bar graphs and place for your head shot and it really detracts and takes away from your resume so I know that people look at RS as a marketing document but it's not really a marketing document in that way give me an example I'll see a resume and it'll say like in a skill section it'll say Excel then it'll have some dots next to it and I'm like what do those dots mean I have no idea it's like an interpretive dance I don't know what it means you don't want to confuse the recruiter or the hiring manager so it's better to just be clear keep it super simple so here are some tips to write a simple and cleanly designed resume we're going to start with fonts my favorite fonts to use are Ariel and calbri I use cbri more than aiel both of them are great I've seen Times New Roman actually making a comeback and it's Chic I dig it consistent layout you need to make sure that you have your headings fonts and spacing are consistent throughout the resume it's going to make it look more polished and more presentable bullet points when you are using bullet points to period or not to period that is the question it's more of a taste thing some people say no periods some people say yes periods it doesn't really matter if you use periods or not what does matter is you're doing it consistently throughout the resume now bold for emphasis sprinkle a little bold around here and there to emphasize things like where you've worked the dates that you've worked Etc consistent contact information so up at the very top of your resume you want to make sure that you're very clear about who you are so you're going to have your name your location your phone pH number your email address your LinkedIn link and any other relevant link so for example if you're a designer if you're in creative and you have outside portfolios include those Links at the very top of your resume and of course spell check proofread true story when I worked at Google one of my hiring managers rejected a candidate and I didn't catch it but the resume had a spelling mistake and the hiring manager literally said this person does not pay attention to detail I will not be considering them take it from me take it from that experience better to spell check save as a PDF save your resume as a PDF when you save your file as a PDF and you are applying to jobs it will maintain the Integrity of the resume so it's going to look the same on your screen as it is going to look the same on the other person's screen now the most important part here are things to avoid so you want to avoid images Graphics any type of clip art columns and tables now the the reason why you want to remove columns and tables is that there are some applicant tracking systems that cannot read those so for example work day cannot read a resume that is more than a single column so if you have a two column resume it's not going to be able to catch all that data and extract that text section eight tailoring you don't have to rewrite your resume every time you apply to a job what we just did is we've created a resume for you that's very targeted to the type of jobs you want to apply for and the type of jobs you want to apply for really have to be in a singular vertical or a singular job family then what you do is when you did that research like you did at the very beginning before you wrote your resume now you're going to go to those job postings now you're going to look at those and you are going to put the job posting next to your resume and you're going to look at them side by side and you are going to see what parts of your resume are the same as the job description and what things are different and of those things that are different you're going to include clude those into your resume and make sure that it is very crystal clear on your resume that you are a fit for that position now a common misconception that I need to talk about are applicant tracking systems applicant tracking systems are also known as ATS systems companies will license an applicant tracking system and what this is It's a digital filing cabinet and it's built on really old Legacy technology that's 20 years old so when you are applying to a role what what the recruiter has done is they've gone into the applicant tracking system and through the system they're able to post the job on their careers page and several external job boards let's say LinkedIn or indeed now the mistake that a lot of people make is they'll say I need to find all the keywords in a job description and I just need to throw them in my resume and we call that keyword stuffing that does not work because people think I need to beat an ATS listen the ATS is a digital filing cabinet there ain't nothing to beat you mind want to try to kick it down the road but I ain't going to work so the reality is this when you apply to a position we use First in first out logic so when you apply to a job if you are the first one to apply your resume will show up first now when you apply to a job on the technical side of things here's what happens when you apply to a job you apply to a job you basically get a profile that's created in the applicant tracking system for you and it'll show the jobs that you've applied for so when I is a recruit go to pull up your application it's going to show me your resume and any other input that you manually inputed the questions about like your racial identity thing your veteran status disability status that is actually not something that recruiters can see that is information that goes directly to the Department of Labor those just don't show up there are a lot of career coaches people who have never actually worked in recruiting who will say things like throw as many keywords into your resume as possible so that way you can beat the ATS or the worst one I've ever seen is they'll say take the job description copy it paste it into your resume and put it in white font so that way the comp the the Bots can read it the AI can read it like no that just makes your resume look silly like even if you put it in point4 font that just doesn't work come on that is not what is happening instead of stuffing your resume with keywords what you want to do is you do want to recognize what those keywords are but you want to Pepper them throughout your resume thoughtfully for the human reader because it's a human that's actually going to read and scan your resume true story I once had a resume that was submitted for a marketing director it was over 20 pages long two Z the resume had all these keywords stuffed in the border on the right hand side it was that was the weirdest resume I've ever seen in my life let's just not do that here's the secret to avoid the biggest mistake the truth is that as I was in the process of editing my video I realized that the secret that I gave you wasn't actually the best secret so here's the best secret what I want you to do is picture this let's look at this analogy imagine you made dinner reservations at a restaurant and you go to the restaurant and you sit down and when you sit down the waiter gives you a menu and the menu says stainless steel utensils Teakwood Furniture porcelain plates you'd be confused right and you're confused because you think yeah of course you have these are basic expectations because you're at a restaurant it's not like they're going to go make food and put the food in your hand they're going to serve it to you on a plate and give you utensils to eat the food with and you see this a lot with resum\\u00e9s where people will write things like pays attention to detail excellent Communicator works well in teams remember these are just basic expectations when you work in a job it's not something that you need to put on your resume what we want to do is you want to think of the job description as the menu that you're receiving so when you get that menu what dish do you want to order so study the menu carefully and understand what that position calls for then describe how your background is that succulent delicious dish that they've been craving so don't sell the plate and the silverware sell the sizzle I just realize I didn't have my camera on what\",\n          \"hi folks my name is Aaron mcgoff I'm a filmmaker and content creator living in New York and I have helped literally millions of people nail their job interviews and today I'm here to help [Music] you so if you clicked on this video I'm going to assume that you have a job interview coming up and if that's the case let me be the first person to congratulate you getting to the interview stage is a huge accomplishment not only did you write a stellar enough application to get through the applicant tracking system and in front of a real human being but they also looked at your and thought Yep this person might be the one let's bring them in for an interview companies don't waste their time interviewing people that they don't think might be the perfect fit for the role so pat yourself on the back it's a huge accomplishment just to get an interview okay so they already love you but now we just got to seal the deal and if you're already watching this video and putting in time and effort to prepare for your interview I feel like you actually have nothing to worry about it seems like this company would be lucky to have you but regardless I'm going to run you through my six-step process to make sure that you nail this interview Step One is mindset your mindset going into a job interview is so so important most people think of it like this I'm so grateful the company wants to interview me but like what if I go in and I mess up and I'm not what they're looking for and I'm just not good enough for them no no no no no no no excuse me let's flip that around is this company good enough for you is this role something that you're even interested in interviews go both ways okay this company might not be the vibe for you you are not desperate you are not looking for someone to give you a handout to give you a chance you are highly skilled you are a professional you have good character character you have good work ethic you are collaborative you are communicative if this company hired you they would be making a smart choice remember the person who is conducting your interview wants to hire you their job is to hire quality candidates to put in the company if you come in and you are that amazing candidate that they want to hire they look good to their boss they are not looking for reasons to reject you they are looking for reasons to hire you so just walk in there and confirm to them that you are the perfect person for this role because in that case they're down with work for the day they can go home and relax just walk into that interview and be their ideal candidate be the solution to all their problems step two review the job description your first interview at most companies is going to be a filtering interview meaning that you're probably not going to interview with your future boss but rather with somebody from the HR or hiring department they are not going to be super familiar with the ins and outs of your role they're probably not going to understand a lot of the jargon that you would use on your daily job and they're honestly not looking to be impressed by your thorough skills or knowledge what they are looking for is to check off boxes on a list most of those boxes are keywords they are listening to hear if you repeat the keywords in the job description so if the job is looking for an editor who knows Premiere Pro and Avid you better say I know Premiere Pro and Avid very well and they're going to go check check you know and other things on the checklist are is this person a psychopath no does this person smell weird no did this person show up on time yes so all you need to do is help them check off their little boxes so before your first interview I want you to go back and reread the job description and then take the 10 15 most common job interview questions and script out your answers for each one using words in the job description then you want to practice them through enough so that you don't sound scripted but that you sound prepared and if you need some extra help I'll link to my job interview Q&A templates down in the description also Pro tip sometimes companies will ask very specific questions if you feel like you're so in the dark and you have no idea what this company's going to ask you I highly recommend going to glore.com looking up the company clicking on interviews and then people will write reports of their interview experience at that company and sometimes people will even write specific questions that they were asked so that's a great way to kind of get an idea of what kind of questions they're going to ask in the interview another Pro tip is that you want to have a lot of stories prepared so companies might ask you what was a time that you overcame an obstacle tell me about a time that you achieved something at tell me about a time that you disagreed with a coworker or my favorite one tell me about a time you failed gosh I hate that question again I have templated answers to all of these questions on my Tik Tok and Instagram so be sure to follow me there step three is research the next step involves heavily researching the company but this shouldn't take you more than 30 minutes to an hour the first place that you want to start is the company website you want to go and read their about or their mission or their value or their careers page you want to get a sense of their values as a company so that you can use those in your answers also the careers page can be a great place to learn about the benefits that they publicly offer so that if you're curious about their for onek match or their paid maternity leave you can see if you can find the answers there and then ask about them in the interview that's a really delicate way of going about that next you're going to want to go over to Google and type in the company name and then click on the news tab read about any recent news for the company or for the industry as a whole did the company recently win any awards did the company recently have a merger was there a wave of layoff This research will truly come in handy when you have to ask questions at the end of the interview which we're going to talk about and lastly you're going to want to go over to my friend LinkedIn if you don't already have a LinkedIn profile definitely create one it's super easy and a great great tool to have if you know the name of the person who is interviewing you feel free to look them up and browse their profile a quick tip if you don't know LinkedIn that well but whenever you look at somebody's profile they will actually receive a notification that you looked at their profile so definitely don't go around sleuthing your ex's profiles because guess they will know but don't be afraid to look at the person who's going to be interviewing you because one they might not check LinkedIn so they might not even know where care but two if they do see it they'll think wow this person is very proactive they're doing their research that's a good thing but I don't know about y'all with LinkedIn but I don't remember the last time that like I actually looked at who looked at my profile maybe I maybe I should that maybe that would be maybe that would be interesting but it's not super common after that go over and type in the company name into LinkedIn and then you can browse the company's page I recommend clicking on the people Tab and browsing the people that work there see if anybody who works there maybe has a mutual connection with you or maybe if they went to the same University or were in the same club or from the same Hometown and if you find somebody who does have a similar connection with you please message them click connect and then click add note and then just say something like hi Adam my name is Aaron I also went to American University I graduated in 2017 I'm interviewing at your company next week do you have any tips for me guys an inside referral is a super weapon when it comes to getting a job having somebody on the inside vouch for you is like 10 extra super points in a video game I don't know if that was a good metaphor but it's really powerful it's like supercharging your application like I tell everybody the best way to get a job the best way to get a good job is to get an internal referral anyway getting distracted back to the Steps step four prepare questions at the end of the interview they're going to ask you so do you have any questions for us and the only wrong answer is no you should always have about four to five questions prepared for the end of the interview the more specific the questions the better some questions like who will I be reporting to in this role or what's the single most important soft skill that someone in this role needs to succeed if you need more inspiration for amazing questions to ask go watch this video that I uploaded and I have a ton of examples for you but again the more specific the question the better for example I recently saw that your competitor released a sustainable product line will this company be following suit questions that are really specific And Timely and show that you're thinking about the company as a whole are like golden step five we're almost there I promise prepare your interview outfit one of the most simple rules for dressing and job interviews is that you can't be overdressed but you can be underdressed so it's always better to air on the side of overdressed what you're wearing doesn't matter as much as your hygiene so you need to make sure that you don't smell your clothes are stain free they're wrinkle-free and they are tailored for your body now of course what you wear varies per industry if you are interviewing for a legal firm you're going to want to wear something more conservative like a black gray or brown suit however if you're interviewing for a creative position at a tech company where everybody wears their pajamas then maybe business casual is best maybe some nice black jeans and a blazer if you don't know what to wear the general rule is that you wear basically One Step Up from what the employees of the company wear so if the companies wear a uniform you would want to wear business casual if the employees were business casual then you would want to wear a business formal and if the employees are business formal then you would want to wear a tuxedo just kidding just kidding that's the last one it ends up business formal and it's almost always safe to wear something between business casual and business formal like a nice relaxed suit or a simple business dress is always a good idea you always want to air on the side of modesty and conservative because you never know who's going to be interviewing you and you don't want to bet against yourself if you're wondering where to shop some good budget options are TJ Maxx Kohl's Marshalls Target H&M is actually really fantastic and then if you're looking for more mid-range I personally really like everlane and abber cromi but some other good ones are Nordstrom Nordstrom wre Banana Republic bobos and Taylor Express and then you know big department stores like Macy's but it's really important that whatever you do buy needs to fit you the fit is almost more important than the clothes if you're wearing something that's too baggy or too tight it might communicate to the employer that you are unprofessional and of course we don't want that because you are professional and you're desirable and they'd be lucky to have you remember okay and the last step step six do a mock interview okay so lastly if you have time I highly recommend doing a mock interview with a family member or a friend but it's important it has to be somebody who's not afraid to be honest with you and give you real feedback so don't do it with your mom who's just going to say oh honey that was so great like no Mom we're not doing that do it with somebody who's going to roast you do it with somebody who's going to say you were fidgeting that whole time you were playing with your hair you didn't make eye contact with me one time I want you to go through the whole thing get dressed up go in no notes have them ask you the questions you have to give the answers and then each time I want them to give you feedback and tell you the truth because research says that 65% of what we communicate is through our body language it's so important to gain self-awareness and the only way to really do that is by talking to somebody else who can reveal to us our own blind spots and behaviors all right that's it for me today I am so so excited please let me know how it goes down in the comments and save this video for future use you know share it with your nieces or nephew or your friends remember you got this and I'll see you next time [Music]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Chunking + LangChain Documents on Top the Your DataFrame"
      ],
      "metadata": {
        "id": "6Cc_3H_tg-PY"
      },
      "id": "6Cc_3H_tg-PY"
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert each row in df_videos (video_id, url, transcript) into multiple LangChain Documents with metadata.\n",
        "def df_to_documents(\n",
        "    df: pd.DataFrame,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 150,\n",
        ") -> list[Document]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "\n",
        "    docs: list[Document] = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        video_id = row[\"video_id\"]\n",
        "        url = row[\"url\"]\n",
        "        transcript = row[\"transcript\"]\n",
        "\n",
        "        # Try to fetch some metadata from YouTube\n",
        "        title = author = description = None\n",
        "        try:\n",
        "            yt = YouTube(url)\n",
        "            title = yt.title\n",
        "            author = yt.author\n",
        "            description = yt.description\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Split transcript into chunks\n",
        "        chunks = splitter.split_text(transcript)\n",
        "\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            doc = Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"video_id\": video_id,\n",
        "                    \"url\": url,\n",
        "                    \"title\": title,\n",
        "                    \"author\": author,\n",
        "                    \"description\": description,\n",
        "                    \"chunk_index\": idx,\n",
        "                },\n",
        "            )\n",
        "            docs.append(doc)\n",
        "\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "DQAyQkdpg9LU"
      },
      "id": "DQAyQkdpg9LU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df_to_documents(df_videos)\n",
        "print(f\"Created {len(documents)} chunks from {len(df_videos)} videos.\")"
      ],
      "metadata": {
        "id": "cCQfetSEfDHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8979ca76-d1e3-45b7-a5cb-77f76956417c"
      },
      "id": "cCQfetSEfDHi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 206 chunks from 10 videos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a LangChain VectorStore (Chroma) from Documents"
      ],
      "metadata": {
        "id": "ZAM2QMuAmBjJ"
      },
      "id": "ZAM2QMuAmBjJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def build_vectorstore_from_documents(\n",
        "    docs: list[Document],\n",
        "    collection_name: str = \"youtube_rag\",\n",
        "    persist_directory: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a Chroma vector store from LangChain Documents.\n",
        "    Uses OpenAI embeddings by default.\n",
        "    \"\"\"\n",
        "    # OpenAI embedding model\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "        collection_name=collection_name,\n",
        "        persist_directory=persist_directory,  # can be None for in-memory\n",
        "    )\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "23CYzLAnl0ly"
      },
      "id": "23CYzLAnl0ly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = build_vectorstore_from_documents(\n",
        "    documents,\n",
        "    collection_name=\"youtube_rag\",\n",
        "    persist_directory=\"./chroma_youtube_rag\",\n",
        ")"
      ],
      "metadata": {
        "id": "TOeHGCPpmCkR"
      },
      "id": "TOeHGCPpmCkR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM + Retriever + Memory"
      ],
      "metadata": {
        "id": "Od7TS69bS2ud"
      },
      "id": "Od7TS69bS2ud"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant answering questions about the content \"\n",
        "            \"of YouTube videos indexed in a vector database. \"\n",
        "            \"Use the retrieved context to answer accurately.\"\n",
        "        ),\n",
        "        (\"human\", \"Context from videos:\\n{context}\\n\\nQuestion: {question}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Lx-_wSeDQaZG"
      },
      "id": "Lx-_wSeDQaZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the RAG pipeline manually"
      ],
      "metadata": {
        "id": "wXCjLu7RS9gP"
      },
      "id": "wXCjLu7RS9gP"
    },
    {
      "cell_type": "code",
      "source": [
        "def youtube_rag_query(question: str):\n",
        "    \"\"\"\n",
        "    Full RAG pipeline:\n",
        "    1. Retrieve relevant video chunks\n",
        "    2. Add them into the prompt\n",
        "    3. Call the LLM\n",
        "    4. Store chat history (memory)\n",
        "    \"\"\"\n",
        "    # ---- Retrieval ----\n",
        "    docs = retriever.invoke(question)\n",
        "\n",
        "    if not docs:\n",
        "        context = \"No relevant content found.\"\n",
        "    else:\n",
        "        context_parts = []\n",
        "        for i, d in enumerate(docs):\n",
        "            meta = d.metadata or {}\n",
        "            context_parts.append(\n",
        "                f\"[{i+1}] Title: {meta.get('title', 'Unknown')}\\n\"\n",
        "                f\"Channel: {meta.get('author', 'Unknown')}\\n\"\n",
        "                f\"Description: {meta.get('description', 'No description')}\\n\"\n",
        "                f\"Chunk {meta.get('chunk_index', '?')}:\\n{d.page_content}\"\n",
        "            )\n",
        "        context = \"\\n\\n---\\n\".join(context_parts)\n",
        "\n",
        "    # ---- Build prompt ----\n",
        "    prompt_msg = rag_prompt.format_messages(\n",
        "        context=context,\n",
        "        question=question,\n",
        "    )\n",
        "\n",
        "    # ---- LLM call ----\n",
        "    response = llm.invoke(prompt_msg)\n",
        "\n",
        "    # ---- Memory update ----\n",
        "    memory.add_message(HumanMessage(content=question))\n",
        "    memory.add_message(response)\n",
        "\n",
        "    return response.content, context\n"
      ],
      "metadata": {
        "id": "EB5XMUeFS5r9"
      },
      "id": "EB5XMUeFS5r9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Tools + Agent with Personality\n",
        "\n",
        "# 1) Personality configuration\n",
        "PERSONALITY = (\n",
        "    \"Friendly, encouraging, and concise. \"\n",
        "    \"Explain things clearly and avoid jargon. \"\n",
        "    \"If the user sounds stressed or insecure, be extra supportive.\"\n",
        ")\n",
        "\n",
        "# 2) Tools\n",
        "\n",
        "@tool\n",
        "def youtube_rag_qa(question: str) -> str:\n",
        "    \"\"\"Answer questions about the YouTube videos that have been ingested into the vector store.\"\"\"\n",
        "    answer, _ctx = youtube_rag_query(question)\n",
        "    return answer\n",
        "\n",
        "@tool\n",
        "def transcribe_audio_file(audio_path: str) -> str:\n",
        "    \"\"\"Transcribe an audio file at a local path to text.\"\"\"\n",
        "    return transcribe_audio_to_text(audio_path)\n",
        "\n",
        "tools = [youtube_rag_qa, transcribe_audio_file]\n",
        "tool_map = {t.name: t for t in tools}\n",
        "\n",
        "# LLM that can call tools\n",
        "tool_llm = llm.bind_tools(tools)\n",
        "\n",
        "# 3) Conversation memory (per session)\n",
        "session_history: Dict[str, List[BaseMessage]] = defaultdict(list)\n",
        "\n",
        "BASE_SYSTEM_PROMPT = (\n",
        "    \"You are an assistant that answers questions about a set of YouTube videos \"\n",
        "    \"that have already been ingested into a vector database.\\n\"\n",
        "    \"If the user asks about the content of the videos, you should use the \"\n",
        "    \"`youtube_rag_qa` tool.\\n\"\n",
        "    \"If the user gives or refers to an audio file path, you can use the \"\n",
        "    \"`transcribe_audio_file` tool to turn it into text and then use \"\n",
        "    \"`youtube_rag_qa` with the transcribed question.\\n\"\n",
        "    \"Always give concise, helpful answers.\"\n",
        ")\n",
        "\n",
        "def build_system_prompt() -> str:\n",
        "    \"\"\"Combine base instructions with the current personality.\"\"\"\n",
        "    if PERSONALITY:\n",
        "        return BASE_SYSTEM_PROMPT + f\"\\n\\nPersonality & tone: {PERSONALITY}\"\n",
        "    return BASE_SYSTEM_PROMPT\n",
        "\n",
        "def agent_chat(user_input: str, session_id: str = \"default\") -> str:\n",
        "    \"\"\"\n",
        "    Simple agent:\n",
        "    - Uses OpenAI tool-calling to decide whether to call youtube_rag_qa / transcribe_audio_file\n",
        "    - Keeps per-session memory of previous turns\n",
        "    - Applies the configured personality on top of the base system prompt\n",
        "    \"\"\"\n",
        "    history = session_history[session_id]\n",
        "\n",
        "    # 1) Build the message list: system + history + new user message\n",
        "    messages: List[BaseMessage] = [\n",
        "        SystemMessage(content=build_system_prompt()),\n",
        "        *history,\n",
        "        HumanMessage(content=user_input),\n",
        "    ]\n",
        "\n",
        "    # 2) First model call: the model may decide to call tools\n",
        "    ai_msg = tool_llm.invoke(messages)\n",
        "    messages.append(ai_msg)\n",
        "\n",
        "    # If the model already answered without tools, just return that\n",
        "    if not getattr(ai_msg, \"tool_calls\", None):\n",
        "        history.extend([HumanMessage(content=user_input), ai_msg])\n",
        "        return ai_msg.content\n",
        "\n",
        "    # 3) If there are tool calls, execute them\n",
        "    tool_messages: List[ToolMessage] = []\n",
        "    for call in ai_msg.tool_calls:\n",
        "        tool_name = call[\"name\"]\n",
        "        tool_args = call[\"args\"]\n",
        "        tool_id = call[\"id\"]\n",
        "\n",
        "        tool = tool_map.get(tool_name)\n",
        "        if tool is None:\n",
        "            tool_result = f\"Error: tool '{tool_name}' not found.\"\n",
        "        else:\n",
        "            # invoke() expects a dict of arguments for the tool\n",
        "            tool_result = tool.invoke(tool_args)\n",
        "\n",
        "        tool_messages.append(\n",
        "            ToolMessage(\n",
        "                content=str(tool_result),\n",
        "                tool_call_id=tool_id,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # 4) Second model call: model sees tool results and produces final answer\n",
        "    messages.extend(tool_messages)\n",
        "    final_ai = llm.invoke(messages)\n",
        "\n",
        "    # 5) Update history with just the human input and final answer\n",
        "    history.extend([\n",
        "        HumanMessage(content=user_input),\n",
        "        final_ai,\n",
        "    ])\n",
        "\n",
        "    return final_ai.content\n"
      ],
      "metadata": {
        "id": "9QFEEFeZa4Ix"
      },
      "id": "9QFEEFeZa4Ix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the RAG pipeline"
      ],
      "metadata": {
        "id": "yu83nRnXuQbd"
      },
      "id": "yu83nRnXuQbd"
    },
    {
      "cell_type": "code",
      "source": [
        "answer, used_context = youtube_rag_query(\"Give me an overview of the videos you indexed.\")\n",
        "\n",
        "print(\"=== ANSWER ===\\n\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\n=== CONTEXT USED ===\\n\")\n",
        "print(used_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV6wuktxTHfX",
        "outputId": "6475fc11-7f71-4b4f-d5c6-7e8dc7bc5c49"
      },
      "id": "wV6wuktxTHfX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ANSWER ===\n",
            "\n",
            "The indexed videos appear to focus on job interview preparation and resume optimization. \n",
            "\n",
            "1. **Interview Questions**: One video emphasizes the importance of having questions prepared to ask at the end of an interview. It suggests that candidates inquire about the company, the role, and the team. The creator has also produced additional content on this topic and offers a free interview preparation checklist in the description.\n",
            "\n",
            "2. **Engagement and Feedback**: Another video encourages viewers to engage with the content by commenting, liking, and subscribing. The creator expresses appreciation for feedback, which helps tailor future video topics to the audience's interests.\n",
            "\n",
            "3. **Resume Optimization**: A third video discusses transforming a resume into a \"highlight reel\" rather than just a list of jobs. It advises viewers to focus on relevant skills and experiences that align with the job they are applying for, emphasizing the importance of using keywords and showcasing achievements effectively.\n",
            "\n",
            "Overall, these videos provide practical advice for job seekers on how to prepare for interviews and enhance their resumes to stand out to potential employers.\n",
            "\n",
            "=== CONTEXT USED ===\n",
            "\n",
            "[1] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 18:\n",
            "more about it more the company more about the role more about the team Etc so you always have questions at the end now I have made a whole slew of videos of questions you can ask at the end of an interview you can start with this one right here and if you need more help I have a free interview prep checklist you can get that in the description box below and I'll see you in the next one bye\n",
            "\n",
            "---\n",
            "[2] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 18:\n",
            "more about it more the company more about the role more about the team Etc so you always have questions at the end now I have made a whole slew of videos of questions you can ask at the end of an interview you can start with this one right here and if you need more help I have a free interview prep checklist you can get that in the description box below and I'll see you in the next one bye\n",
            "\n",
            "---\n",
            "[3] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 16:\n",
            "please comment, like, subscribe, do all that jazz. And I really appreciate the feedback cuz that helps me understand what videos you guys want next and I can do it at an everinccreasing pace on this YouTube channel. So, thanks everyone for watching and I'll see you guys next\n",
            "\n",
            "---\n",
            "[4] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 16:\n",
            "please comment, like, subscribe, do all that jazz. And I really appreciate the feedback cuz that helps me understand what videos you guys want next and I can do it at an everinccreasing pace on this YouTube channel. So, thanks everyone for watching and I'll see you guys next\n",
            "\n",
            "---\n",
            "[5] Title: Unknown\n",
            "Channel: Unknown\n",
            "Description: No description\n",
            "Chunk 4:\n",
            "description of this video or in the show notes of this podcast can snag the toolkit for over 60% off which leads us to step number two and that is to turn your resume into a highlight reel now this is where you make the ATS see that you're not just a good choice but the best choice your resume should be more than just a list of jobs it really needs to be a compelling highlight reel of your professional achievements so the key is to be selective only showcasing the skills and the experiences that closely match the job you're applying for and of course cutting out anything else that doesn't add value so I want you to start by focusing on your most recent and relevant roles highlight each position with bullet points that feature the keywords and skills you identified earlier now remember relevance is King it's easy to want to list everything especially if you have a long work history if it doesn't align with the job you're targeting it's just noise to the ATS for each role craft bullet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat loop\n",
        "\n"
      ],
      "metadata": {
        "id": "RrNr8dr9u-Gu"
      },
      "id": "RrNr8dr9u-Gu"
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_youtube_bot(session_id: str = \"default\"):\n",
        "    \"\"\"\n",
        "    Simple text-based chat loop in Colab, powered by the tool-calling agent + memory.\n",
        "    \"\"\"\n",
        "    print(\"YouTube QA ChatBot (agent powered)\")\n",
        "    print(\"Ask me anything about the videos I have indexed.\")\n",
        "    print(\"Type 'exit' or 'quit' to end the chat.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "            print(\"Bot: Bye!\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        # ✅ Use the agent (tools + memory), not the bare RAG function\n",
        "        answer = agent_chat(user_input, session_id=session_id)\n",
        "        print(f\"Bot: {answer}\\n\")\n",
        "\n",
        "\n",
        "# Run this to start chatting:\n",
        "chat_with_youtube_bot()\n"
      ],
      "metadata": {
        "id": "rQ1KOGBAZDyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d100a4-299e-4a71-db77-8f42199ed774"
      },
      "id": "rQ1KOGBAZDyd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YouTube QA ChatBot (agent powered)\n",
            "Ask me anything about the videos I have indexed.\n",
            "Type 'exit' or 'quit' to end the chat.\n",
            "\n",
            "You: exit\n",
            "Bot: Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speech recognition"
      ],
      "metadata": {
        "id": "GwioHuC-0Tja"
      },
      "id": "GwioHuC-0Tja"
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio_to_text(audio_path: str, model: str = \"gpt-4o-mini-transcribe\") -> str:\n",
        "    \"\"\"\n",
        "    Transcribe an audio file to text using OpenAI's speech recognition.\n",
        "    Adjust model name if needed depending on what your account supports.\n",
        "    \"\"\"\n",
        "    with open(audio_path, \"rb\") as audio_file:\n",
        "        transcription = openai_client.audio.transcriptions.create(\n",
        "            model=model,\n",
        "            file=audio_file,\n",
        "            response_format=\"text\",\n",
        "        )\n",
        "    # For newer clients this is already a string; if it's an object, cast to str\n",
        "    return str(transcription)\n",
        "\n",
        "\n",
        "def tts_from_text(text: str, output_dir: str = \"tts_outputs\") -> str:\n",
        "    \"\"\"\n",
        "    Convert answer text to speech using OpenAI TTS and return the audio file path.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    filename = f\"answer_{uuid.uuid4().hex}.mp3\"\n",
        "    out_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    speech = openai_client.audio.speech.create(\n",
        "        model=\"gpt-4o-mini-tts\",  # or another TTS-capable model\n",
        "        voice=\"alloy\",\n",
        "        input=text,\n",
        "    )\n",
        "\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        f.write(speech.read())\n",
        "\n",
        "    return out_path\n"
      ],
      "metadata": {
        "id": "3bP0GoOnvWVN"
      },
      "id": "3bP0GoOnvWVN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_from_audio_file(audio_path: str, session_id: str = \"voice-session\"):\n",
        "    \"\"\"\n",
        "    Core backend logic:\n",
        "    1. Transcribe audio file at `audio_path` to text\n",
        "    2. Run the text through the agent (which internally uses RAG + tools + memory)\n",
        "    3. Return (transcript, answer)\n",
        "    \"\"\"\n",
        "    transcript_text = transcribe_audio_to_text(audio_path)\n",
        "    answer = agent_chat(transcript_text, session_id=session_id)\n",
        "    return transcript_text, answer\n"
      ],
      "metadata": {
        "id": "82koS--63Gkh"
      },
      "id": "82koS--63Gkh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change for Gradio later"
      ],
      "metadata": {
        "id": "LDoXdxTW97Sy"
      },
      "id": "LDoXdxTW97Sy"
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_audio_qa(audio_path, tts_enabled: bool):\n",
        "    \"\"\"\n",
        "    Gradio backend:\n",
        "    - Takes an audio question (mic or upload)\n",
        "    - Uses the agent-backed pipeline to get (transcript, answer)\n",
        "    - Optionally generates spoken answer audio if tts_enabled is True\n",
        "    \"\"\"\n",
        "    if audio_path is None:\n",
        "        return \"\", \"No audio received.\", None\n",
        "\n",
        "    # This already calls agent_chat under the hood\n",
        "    transcript, answer = answer_from_audio_file(audio_path)\n",
        "\n",
        "    answer_audio_path = None\n",
        "    if tts_enabled:\n",
        "        try:\n",
        "            answer_audio_path = tts_from_text(answer)\n",
        "        except Exception as e:\n",
        "            print(\"TTS error:\", e)\n",
        "\n",
        "    return transcript, answer, answer_audio_path\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🎬 YouTube QA Bot with Voice Input\")\n",
        "\n",
        "    with gr.Row():\n",
        "        audio_input = gr.Audio(\n",
        "            sources=[\"microphone\"],   # 👈 mic only\n",
        "            type=\"filepath\",          # pass a file path to backend\n",
        "            label=\"Hold to record your question\",\n",
        "        )\n",
        "        tts_toggle = gr.Checkbox(\n",
        "            label=\"Read answer aloud\",\n",
        "            value=False,\n",
        "        )\n",
        "\n",
        "    transcript_output = gr.Textbox(\n",
        "        label=\"Transcribed question\",\n",
        "        lines=2,\n",
        "    )\n",
        "    answer_output = gr.Textbox(\n",
        "        label=\"Bot answer\",\n",
        "        lines=4,\n",
        "    )\n",
        "    answer_audio_output = gr.Audio(\n",
        "        label=\"Spoken answer\",\n",
        "        type=\"filepath\",\n",
        "    )\n",
        "\n",
        "    audio_input.change(\n",
        "        fn=gradio_audio_qa,\n",
        "        inputs=[audio_input, tts_toggle],\n",
        "        outputs=[transcript_output, answer_output, answer_audio_output],\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "RD3CLyAp98Hr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "outputId": "bb9371c6-ba22-4e0c-81e9-24eb283d5d24"
      },
      "id": "RD3CLyAp98Hr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6556af2ff22a15e023.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6556af2ff22a15e023.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ Gradio backend wrappers ------------\n",
        "\n",
        "def gr_text_chat(message, history):\n",
        "    \"\"\"\n",
        "    Backend for the text chat tab.\n",
        "\n",
        "    message: latest user message (string)\n",
        "    history: list of (user, bot) tuples for the Chatbot component\n",
        "    \"\"\"\n",
        "    if not message:\n",
        "        return history, \"\"  # nothing to do\n",
        "\n",
        "    # Use your agent with tools + memory\n",
        "    answer = agent_chat(message, session_id=\"gradio-text-session\")\n",
        "\n",
        "    # Append to visible chat history\n",
        "    history = history + [(message, answer)]\n",
        "    # Return new history and clear the input box\n",
        "    return history, \"\"\n",
        "\n",
        "\n",
        "def gradio_audio_qa(audio_path, tts_enabled: bool):\n",
        "    \"\"\"\n",
        "    Gradio backend for the audio tab.\n",
        "\n",
        "    - Takes an audio question (file path from mic/upload)\n",
        "    - Uses your backend to get (transcript, answer)\n",
        "    - Optionally generates spoken answer audio if tts_enabled is True\n",
        "    \"\"\"\n",
        "    if audio_path is None:\n",
        "        return \"\", \"No audio received.\", None\n",
        "\n",
        "    # This already uses agent_chat under the hood\n",
        "    transcript, answer = answer_from_audio_file(\n",
        "        audio_path,\n",
        "        session_id=\"gradio-audio-session\",\n",
        "    )\n",
        "\n",
        "    answer_audio_path = None\n",
        "    if tts_enabled:\n",
        "        try:\n",
        "            answer_audio_path = tts_from_text(answer)\n",
        "        except Exception as e:\n",
        "            print(\"TTS error:\", e)\n",
        "            answer_audio_path = None\n",
        "\n",
        "    return transcript, answer, answer_audio_path\n",
        "\n",
        "\n",
        "# ------------ Gradio UI (text + audio tabs) ------------\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🎥 YouTube Job Coach Bot\\nAsk me anything about the indexed career videos.\")\n",
        "\n",
        "    # ===== TAB 1: TEXT CHAT =====\n",
        "    with gr.Tab(\"💬 Text Chat\"):\n",
        "        chatbot = gr.Chatbot(\n",
        "            label=\"Conversation\",\n",
        "            height=400,\n",
        "        )\n",
        "        msg = gr.Textbox(\n",
        "            label=\"Your question\",\n",
        "            placeholder=\"Ask me about CVs, interviews, job search, etc.\",\n",
        "        )\n",
        "        send_btn = gr.Button(\"Send\")\n",
        "\n",
        "        # User presses Enter in the textbox\n",
        "        msg.submit(\n",
        "            fn=gr_text_chat,\n",
        "            inputs=[msg, chatbot],\n",
        "            outputs=[chatbot, msg],\n",
        "        )\n",
        "\n",
        "        # User clicks the button\n",
        "        send_btn.click(\n",
        "            fn=gr_text_chat,\n",
        "            inputs=[msg, chatbot],\n",
        "            outputs=[chatbot, msg],\n",
        "        )\n",
        "\n",
        "    # ===== TAB 2: AUDIO QUESTION =====\n",
        "    with gr.Tab(\"🎙️ Audio Question\"):\n",
        "        gr.Markdown(\n",
        "            \"Record or upload a spoken question. \"\n",
        "            \"The bot will transcribe it, answer using the YouTube RAG pipeline, \"\n",
        "            \"and optionally speak the answer.\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            audio_input = gr.Audio(\n",
        "                sources=[\"microphone\"],   # mic only (you can add 'upload' if you like)\n",
        "                type=\"filepath\",          # pass a file path to backend\n",
        "                label=\"Hold to record your question\",\n",
        "            )\n",
        "            tts_toggle = gr.Checkbox(\n",
        "                label=\"Read answer aloud\",\n",
        "                value=False,\n",
        "            )\n",
        "\n",
        "        transcript_output = gr.Textbox(\n",
        "            label=\"Transcribed question\",\n",
        "            lines=2,\n",
        "        )\n",
        "        answer_output = gr.Textbox(\n",
        "            label=\"Bot answer\",\n",
        "            lines=4,\n",
        "        )\n",
        "        answer_audio_output = gr.Audio(\n",
        "            label=\"Spoken answer\",\n",
        "            type=\"filepath\",\n",
        "        )\n",
        "\n",
        "        # Trigger on new/changed audio\n",
        "        audio_input.change(\n",
        "            fn=gradio_audio_qa,\n",
        "            inputs=[audio_input, tts_toggle],\n",
        "            outputs=[transcript_output, answer_output, answer_audio_output],\n",
        "        )\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "2mxvo0Dk1_Ib",
        "outputId": "07cbb5d4-6a71-4080-e56a-458e810ab3a0"
      },
      "id": "2mxvo0Dk1_Ib",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3214356147.py:57: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-3214356147.py:57: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8fa335814db544b41f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8fa335814db544b41f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Run this once in Colab to install metric libraries)\n",
        "!pip install -q rouge-score sacrebleu"
      ],
      "metadata": {
        "id": "NyoLISCx6k0X"
      },
      "id": "NyoLISCx6k0X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1) Tiny evaluation dataset of (question, expected answer) pairs\n",
        "\n",
        "eval_examples = [\n",
        "    {\n",
        "        \"question\": \"What is the main topic of the FAANG interview video?\",\n",
        "        \"expected\": \"The video explains how FAANG interviews typically work and gives tips on how to prepare for them.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"According to the videos, what is one key recommendation for writing your CV or resume?\",\n",
        "        \"expected\": \"You should tailor your CV or resume to each job and keep it clear and concise.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What do the videos suggest you should do after being rejected from a job application?\",\n",
        "        \"expected\": \"They recommend treating rejection as normal, asking for feedback when possible, and using it to improve your preparation.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How do the videos describe the benefit of preparing behavioral stories in advance?\",\n",
        "        \"expected\": \"Preparing behavioral stories in advance helps you answer interview questions more confidently and clearly, often using the STAR method.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is one tip mentioned about researching a company before an interview?\",\n",
        "        \"expected\": \"You should research the company's products and culture so you can tailor your answers and show genuine interest.\"\n",
        "    },\n",
        "    # You can add or edit examples here (aim for 5–10 total).\n",
        "]\n",
        "\n",
        "\n",
        "# ✅ 2) Semantic similarity evaluation with embeddings\n",
        "\n",
        "# Reuse the same embedding model type as your vectorstore\n",
        "eval_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "\n",
        "def evaluate_example_semantic(example):\n",
        "    question = example[\"question\"]\n",
        "    expected = example[\"expected\"]\n",
        "\n",
        "    # Use your existing RAG function (must already be defined earlier)\n",
        "    answer, _ctx = youtube_rag_query(question)\n",
        "\n",
        "    exp_vec = eval_embeddings.embed_query(expected)\n",
        "    ans_vec = eval_embeddings.embed_query(answer)\n",
        "\n",
        "    sim = cosine_sim(exp_vec, ans_vec)\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"expected\": expected,\n",
        "        \"answer\": answer,\n",
        "        \"cosine_similarity\": sim,\n",
        "    }\n",
        "\n",
        "\n",
        "semantic_results = [evaluate_example_semantic(ex) for ex in eval_examples]\n",
        "\n",
        "print(\"=== Semantic similarity results ===\")\n",
        "for r in semantic_results:\n",
        "    print(\"Q:\", r[\"question\"])\n",
        "    print(\"Expected:\", r[\"expected\"])\n",
        "    print(\"Answer:\", r[\"answer\"])\n",
        "    print(\"Cosine similarity:\", round(r[\"cosine_similarity\"], 3))\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "avg_sim = sum(r[\"cosine_similarity\"] for r in semantic_results) / len(semantic_results)\n",
        "print(\"✅ Average semantic similarity:\", round(avg_sim, 3))\n",
        "\n",
        "\n",
        "# ✅ 3) BLEU & ROUGE-L metrics\n",
        "\n",
        "rouge_scorer_obj = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def eval_bleu_rouge(expected: str, answer: str):\n",
        "    \"\"\"\n",
        "    Compute BLEU and ROUGE-L between a reference answer and the model answer.\n",
        "    BLEU is reported as a percentage (0–100), ROUGE-L as F1 (0–1).\n",
        "    \"\"\"\n",
        "    # sacrebleu expects [hypotheses], [[references]]\n",
        "    bleu = sacrebleu.corpus_bleu([answer], [[expected]]).score\n",
        "\n",
        "    rouge_scores = rouge_scorer_obj.score(expected, answer)\n",
        "    rouge_l = rouge_scores[\"rougeL\"].fmeasure\n",
        "\n",
        "    return bleu, rouge_l\n",
        "\n",
        "\n",
        "# ✅ 4) Evaluate all examples with BLEU + ROUGE-L\n",
        "\n",
        "def evaluate_example_with_metrics(example):\n",
        "    question = example[\"question\"]\n",
        "    expected = example[\"expected\"]\n",
        "\n",
        "    answer, _ctx = youtube_rag_query(question)\n",
        "\n",
        "    bleu, rouge_l = eval_bleu_rouge(expected, answer)\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"expected\": expected,\n",
        "        \"answer\": answer,\n",
        "        \"bleu\": bleu,\n",
        "        \"rougeL\": rouge_l,\n",
        "    }\n",
        "\n",
        "metric_results = [evaluate_example_with_metrics(ex) for ex in eval_examples]\n",
        "\n",
        "print(\"\\n=== BLEU & ROUGE-L results ===\")\n",
        "for r in metric_results:\n",
        "    print(\"Q:\", r[\"question\"])\n",
        "    print(\"BLEU:\", round(r[\"bleu\"], 2), \"ROUGE-L:\", round(r[\"rougeL\"], 3))\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "avg_bleu = sum(r[\"bleu\"] for r in metric_results) / len(metric_results)\n",
        "avg_rougeL = sum(r[\"rougeL\"] for r in metric_results) / len(metric_results)\n",
        "\n",
        "print(\"✅ Average BLEU:\", round(avg_bleu, 2))\n",
        "print(\"✅ Average ROUGE-L:\", round(avg_rougeL, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcg-qlWs-CCv",
        "outputId": "91a1f43e-5f02-4abd-e452-5dce580db48a"
      },
      "id": "qcg-qlWs-CCv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Semantic similarity results ===\n",
            "Q: What is the main topic of the FAANG interview video?\n",
            "Expected: The video explains how FAANG interviews typically work and gives tips on how to prepare for them.\n",
            "Answer: The main topic of the FAANG interview video revolves around the evolving landscape of tech interviews, particularly within major tech companies like Google, Amazon, and Meta. The video discusses how these companies are adapting their interview processes in response to changes in technology and candidate behavior, including the use of AI and advanced surveillance techniques during interviews. It highlights three major trends reshaping hiring practices, the shift from traditional in-person interviews to video interviews, and the challenges companies face in maintaining the integrity of their interview processes.\n",
            "Cosine similarity: 0.717\n",
            "--------------------------------------------------------------------------------\n",
            "Q: According to the videos, what is one key recommendation for writing your CV or resume?\n",
            "Expected: You should tailor your CV or resume to each job and keep it clear and concise.\n",
            "Answer: One key recommendation for writing your CV or resume is to turn it into a \"highlight reel\" of your professional achievements. This means being selective and only showcasing the skills and experiences that closely match the job you are applying for, while cutting out anything that doesn't add value. It's important to focus on your most recent and relevant roles and to craft bullet points that feature the keywords and skills identified in the job description. This approach helps make your resume compelling and tailored for each application.\n",
            "Cosine similarity: 0.587\n",
            "--------------------------------------------------------------------------------\n",
            "Q: What do the videos suggest you should do after being rejected from a job application?\n",
            "Expected: They recommend treating rejection as normal, asking for feedback when possible, and using it to improve your preparation.\n",
            "Answer: The videos suggest that if you are getting rejections from job applications, it is likely not due to your qualifications but rather because you are not effectively communicating your results. To improve your chances, you should focus on transforming your resume to better highlight your achievements. Additionally, it's important to approach your job search with a gentle mindset, ensuring that you prepare thoroughly for interviews by researching the company and the people you will meet. Engaging with interviewers and asking insightful questions can also help you stand out. If you need further support, consider booking a call with a professional to explore how they can assist you in your job search.\n",
            "Cosine similarity: 0.516\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How do the videos describe the benefit of preparing behavioral stories in advance?\n",
            "Expected: Preparing behavioral stories in advance helps you answer interview questions more confidently and clearly, often using the STAR method.\n",
            "Answer: The videos emphasize that preparing behavioral stories in advance allows candidates to deliver their responses with confidence and conviction during interviews. By having multiple stories that demonstrate key qualities, candidates can effectively communicate their experiences using the STAR framework (Situation, Task, Action, Result). This preparation not only helps in articulating responses clearly but also leaves a positive impression on interviewers, making them more likely to want to hire the candidate. Additionally, the videos suggest that synthesizing lessons learned from these experiences can further enhance the sharpness of the interview responses.\n",
            "Cosine similarity: 0.846\n",
            "--------------------------------------------------------------------------------\n",
            "Q: What is one tip mentioned about researching a company before an interview?\n",
            "Expected: You should research the company's products and culture so you can tailor your answers and show genuine interest.\n",
            "Answer: One tip mentioned about researching a company before an interview is to start by visiting the company's website. You should read their \"About\" section, mission statement, values, and careers page to understand their values and benefits. This information can help you tailor your answers during the interview and prepare relevant questions to ask. Additionally, it's suggested to check recent news about the company to stay informed about any significant developments, which can also be useful during the interview.\n",
            "Cosine similarity: 0.661\n",
            "--------------------------------------------------------------------------------\n",
            "✅ Average semantic similarity: 0.665\n",
            "\n",
            "=== BLEU & ROUGE-L results ===\n",
            "Q: What is the main topic of the FAANG interview video?\n",
            "BLEU: 1.11 ROUGE-L: 0.122\n",
            "--------------------------------------------------------------------------------\n",
            "Q: According to the videos, what is one key recommendation for writing your CV or resume?\n",
            "BLEU: 2.91 ROUGE-L: 0.15\n",
            "--------------------------------------------------------------------------------\n",
            "Q: What do the videos suggest you should do after being rejected from a job application?\n",
            "BLEU: 1.04 ROUGE-L: 0.092\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How do the videos describe the benefit of preparing behavioral stories in advance?\n",
            "BLEU: 3.23 ROUGE-L: 0.169\n",
            "--------------------------------------------------------------------------------\n",
            "Q: What is one tip mentioned about researching a company before an interview?\n",
            "BLEU: 2.24 ROUGE-L: 0.171\n",
            "--------------------------------------------------------------------------------\n",
            "✅ Average BLEU: 2.11\n",
            "✅ Average ROUGE-L: 0.141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Wrap youtube_rag_query in a Runnable for LangSmith\n",
        "\n",
        "def rag_run(inputs: dict) -> str:\n",
        "    \"\"\"\n",
        "    Wrapper for youtube_rag_query so LangSmith can call it easily.\n",
        "    Expects inputs like {\"question\": \"...\"} and returns the answer text.\n",
        "    \"\"\"\n",
        "    question = inputs[\"question\"]\n",
        "    answer, _ctx = youtube_rag_query(question)\n",
        "    return answer\n",
        "\n",
        "rag_runnable = RunnableLambda(rag_run)"
      ],
      "metadata": {
        "id": "eAagyspV-GGq"
      },
      "id": "eAagyspV-GGq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ LangSmith evaluation: create dataset if missing, then run evaluation\n",
        "\n",
        "client = Client()\n",
        "dataset_name = \"youtube-qa-eval\"\n",
        "\n",
        "# 1) Ensure dataset exists (create if not)\n",
        "if client.has_dataset(dataset_name=dataset_name):\n",
        "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
        "    print(f\"Dataset '{dataset_name}' already exists.\")\n",
        "else:\n",
        "    print(f\"Dataset '{dataset_name}' not found. Creating and populating it...\")\n",
        "    dataset = client.create_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        description=\"Evaluation dataset for YouTube QA bot.\",\n",
        "    )\n",
        "    # Add our eval_examples as dataset entries\n",
        "    for ex in eval_examples:\n",
        "        client.create_example(\n",
        "            inputs={\"question\": ex[\"question\"]},\n",
        "            outputs={\"expected\": ex[\"expected\"]},\n",
        "            dataset_id=dataset.id,\n",
        "        )\n",
        "    print(f\"Created dataset and added {len(eval_examples)} examples.\")\n",
        "\n",
        "# 2) Run evaluation: no automatic evaluators, just log results to LangSmith\n",
        "eval_results = evaluate(\n",
        "    rag_runnable,\n",
        "    data=dataset_name,            # dataset name\n",
        "    experiment_prefix=\"youtube-qa\",\n",
        ")\n",
        "\n",
        "print(\"✅ LangSmith evaluation complete. Check LangSmith (Datasets → Evaluations) for the run outputs and traces.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "73a90e6d97b4435ab3792bfabaa3c6b0",
            "a149dfb5fa0a451ab06fa5ba32aa1d6f",
            "e4b5cfa1a8c445088278c752b3295f0b",
            "e92581cc89b3433396a33d21bc15a0b0",
            "0b505c09e3104a80b2a1b70566a6c850",
            "2d6aeafe013244e085959918c3bd718c",
            "40af58f565fb4f4f9cc31464f51789b5",
            "34d9576c17fe486cb7eae3d87443f4f1",
            "e9aa164c2d1b46389619612acfb8fdc0",
            "800ff3b474b34dcf83d6ec25fd251999",
            "98e0a09287f14324b4a15bc84883d234"
          ]
        },
        "id": "HV1xGbTk-SXU",
        "outputId": "acb99167-19a5-41c9-9a5a-07d7871412aa"
      },
      "id": "HV1xGbTk-SXU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'youtube-qa-eval' already exists.\n",
            "View the evaluation results for experiment: 'youtube-qa-183a331c' at:\n",
            "https://smith.langchain.com/o/08e1da5a-9808-4fe7-b2aa-01d1a94c2d6b/datasets/46474ce6-a236-4c77-b7a9-a0e7bf2e9acd/compare?selectedSessions=f13c01dc-4720-45af-b211-9127ac524c67\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73a90e6d97b4435ab3792bfabaa3c6b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LangSmith evaluation complete. Check LangSmith (Datasets → Evaluations) for the run outputs and traces.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHp4jM8eAVna"
      },
      "id": "QHp4jM8eAVna",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73a90e6d97b4435ab3792bfabaa3c6b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a149dfb5fa0a451ab06fa5ba32aa1d6f",
              "IPY_MODEL_e4b5cfa1a8c445088278c752b3295f0b",
              "IPY_MODEL_e92581cc89b3433396a33d21bc15a0b0"
            ],
            "layout": "IPY_MODEL_0b505c09e3104a80b2a1b70566a6c850"
          }
        },
        "a149dfb5fa0a451ab06fa5ba32aa1d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d6aeafe013244e085959918c3bd718c",
            "placeholder": "​",
            "style": "IPY_MODEL_40af58f565fb4f4f9cc31464f51789b5",
            "value": ""
          }
        },
        "e4b5cfa1a8c445088278c752b3295f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34d9576c17fe486cb7eae3d87443f4f1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9aa164c2d1b46389619612acfb8fdc0",
            "value": 1
          }
        },
        "e92581cc89b3433396a33d21bc15a0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_800ff3b474b34dcf83d6ec25fd251999",
            "placeholder": "​",
            "style": "IPY_MODEL_98e0a09287f14324b4a15bc84883d234",
            "value": " 5/? [00:16&lt;00:00,  3.11s/it]"
          }
        },
        "0b505c09e3104a80b2a1b70566a6c850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6aeafe013244e085959918c3bd718c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40af58f565fb4f4f9cc31464f51789b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34d9576c17fe486cb7eae3d87443f4f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e9aa164c2d1b46389619612acfb8fdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "800ff3b474b34dcf83d6ec25fd251999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e0a09287f14324b4a15bc84883d234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}